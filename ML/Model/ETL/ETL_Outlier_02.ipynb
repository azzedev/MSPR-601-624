{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2bf117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook exécuté depuis : /app/Model/ETL\n",
      "PROC_DATA_DIR  : /app/Model/ProcessedData\n",
      "PROC_DATA_DIR2 : /app/Model/ProcessedData2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd, numpy as np, matplotlib.pyplot as plt, seaborn as sns\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "try:\n",
    "    BASE_DIR = Path(__file__).resolve().parent\n",
    "except NameError:\n",
    "    BASE_DIR = Path.cwd()\n",
    "\n",
    "print(\"Notebook exécuté depuis :\", BASE_DIR)\n",
    "\n",
    "PROJECT_ROOT   = BASE_DIR.parent\n",
    "\n",
    "PROC_DATA_DIR  = PROJECT_ROOT / \"ProcessedData\"\n",
    "PROC_DATA_DIR2 = PROJECT_ROOT / \"ProcessedData2\" \n",
    "PROC_DATA_DIR2.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"PROC_DATA_DIR  :\", PROC_DATA_DIR)\n",
    "print(\"PROC_DATA_DIR2 :\", PROC_DATA_DIR2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91e3297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions du dataset: (421264, 22)\n",
      "\n",
      "colonnes disponibles:\n",
      "['iso_code', 'continent', 'location', 'date', 'new_cases', 'total_cases', 'new_cases_smoothed', 'new_cases_per_million', 'new_cases_smoothed_per_million', 'reproduction_rate', 'new_deaths', 'total_deaths', 'new_deaths_smoothed', 'new_deaths_per_million', 'new_deaths_smoothed_per_million', 'total_deaths_per_million', 'mortality_rate', 'population', 'population_density', 'stringency_index', 'latitude', 'longitude']\n",
      "\n",
      "plage de dates: 2020-01-01 00:00:00 a 2024-08-14 00:00:00\n",
      "\n",
      "nombre de pays uniques: 243\n",
      "\n",
      "taux de valeurs manquantes pour les colonnes cles:\n",
      "  new_cases: 3.0%\n",
      "  total_cases: 2.6%\n",
      "  new_deaths: 2.9%\n",
      "  total_deaths: 2.6%\n",
      "  reproduction_rate: 53.6%\n",
      "  mortality_rate: 9.7%\n",
      "  population: 0.0%\n",
      "  stringency_index: 50.8%\n"
     ]
    }
   ],
   "source": [
    "# importation des bibliotheques necessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# configuration de l'affichage\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# chargement des donnees covid pays \n",
    "covid_data = pd.read_csv(PROC_DATA_DIR / 'covid_countries_clean.csv')\n",
    "covid_data['date'] = pd.to_datetime(covid_data['date'])\n",
    "\n",
    "# affichage des informations de base\n",
    "print(\"dimensions du dataset:\", covid_data.shape)\n",
    "print(\"\\ncolonnes disponibles:\")\n",
    "print(covid_data.columns.tolist())\n",
    "print(\"\\nplage de dates:\", covid_data['date'].min(), \"a\", covid_data['date'].max())\n",
    "print(\"\\nnombre de pays uniques:\", covid_data['location'].nunique())\n",
    "\n",
    "# verification des valeurs manquantes pour les colonnes cles\n",
    "colonnes_cles = ['new_cases', 'total_cases', 'new_deaths', 'total_deaths', \n",
    "                 'reproduction_rate', 'mortality_rate', 'population', 'stringency_index']\n",
    "print(\"\\ntaux de valeurs manquantes pour les colonnes cles:\")\n",
    "for col in colonnes_cles:\n",
    "    if col in covid_data.columns:\n",
    "        taux_nan = covid_data[col].isna().sum() / len(covid_data) * 100\n",
    "        print(f\"  {col}: {taux_nan:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "478bff0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== analyse de la distribution des new_cases ===\n",
      "\n",
      "statistiques de base pour new_cases:\n",
      "count     408485.000\n",
      "mean        2080.297\n",
      "std        84712.729\n",
      "min            0.000\n",
      "25%            0.000\n",
      "50%            0.000\n",
      "75%            0.000\n",
      "max     40475477.000\n",
      "Name: new_cases, dtype: float64\n",
      "\n",
      "repartition des valeurs:\n",
      "  valeurs zero: 367445 (87.2%)\n",
      "  valeurs nan: 12779 (3.0%)\n",
      "  valeurs positives: 41040 (9.7%)\n",
      "\n",
      "=== analyse par jour de la semaine ===\n",
      "              moyenne  mediane  nombre_observations  nombre_zeros  pct_zeros\n",
      "day_of_week                                                                 \n",
      "Monday          0.000    0.000                58323         58323    100.000\n",
      "Tuesday         0.000    0.000                58323         58323    100.000\n",
      "Wednesday       0.000    0.000                58323         58323    100.000\n",
      "Thursday        0.000    0.000                58323         58323    100.000\n",
      "Friday          0.000    0.000                58323         58323    100.000\n",
      "Saturday        0.000    0.000                58323         58323    100.000\n",
      "Sunday      14514.320   62.000                58547         17507     29.900\n"
     ]
    }
   ],
   "source": [
    "# analyse de la distribution des new_cases\n",
    "print(\"=== analyse de la distribution des new_cases ===\")\n",
    "\n",
    "# statistiques de base\n",
    "print(\"\\nstatistiques de base pour new_cases:\")\n",
    "print(covid_data['new_cases'].describe())\n",
    "\n",
    "# analyse des zeros\n",
    "total_rows = len(covid_data)\n",
    "zero_cases = (covid_data['new_cases'] == 0).sum()\n",
    "nan_cases = covid_data['new_cases'].isna().sum()\n",
    "positive_cases = (covid_data['new_cases'] > 0).sum()\n",
    "\n",
    "print(f\"\\nrepartition des valeurs:\")\n",
    "print(f\"  valeurs zero: {zero_cases} ({zero_cases/total_rows*100:.1f}%)\")\n",
    "print(f\"  valeurs nan: {nan_cases} ({nan_cases/total_rows*100:.1f}%)\")\n",
    "print(f\"  valeurs positives: {positive_cases} ({positive_cases/total_rows*100:.1f}%)\")\n",
    "\n",
    "# analyse par jour de la semaine\n",
    "covid_data['day_of_week'] = covid_data['date'].dt.day_name()\n",
    "covid_data['week_number'] = covid_data['date'].dt.isocalendar().week\n",
    "\n",
    "# moyenne des nouveaux cas par jour de la semaine\n",
    "print(\"\\n=== analyse par jour de la semaine ===\")\n",
    "day_analysis = covid_data.groupby('day_of_week').agg({\n",
    "    'new_cases': ['mean', 'median', 'count', lambda x: (x == 0).sum()]\n",
    "}).round(2)\n",
    "day_analysis.columns = ['moyenne', 'mediane', 'nombre_observations', 'nombre_zeros']\n",
    "day_analysis['pct_zeros'] = (day_analysis['nombre_zeros'] / day_analysis['nombre_observations'] * 100).round(1)\n",
    "\n",
    "# reordonner les jours\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_analysis = day_analysis.reindex(day_order)\n",
    "print(day_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98b5e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== analyse du pattern de rapport par pays ===\n",
      "\n",
      "patterns de rapport pour 20 pays (echantillon):\n",
      "               country report_day  report_day_zero_pct  autres_jours_zero_pct\n",
      "0          Afghanistan     Sunday                9.167                100.000\n",
      "1              Albania     Sunday               21.250                100.000\n",
      "2              Algeria     Sunday               13.333                100.000\n",
      "3       American Samoa     Sunday               75.833                100.000\n",
      "4              Andorra     Sunday               34.167                100.000\n",
      "..                 ...        ...                  ...                    ...\n",
      "238  Wallis and Futuna     Sunday               90.000                100.000\n",
      "239     Western Sahara  Wednesday                0.000                    NaN\n",
      "240              Yemen     Sunday               49.167                100.000\n",
      "241             Zambia     Sunday               19.583                100.000\n",
      "242           Zimbabwe     Sunday               11.667                100.000\n",
      "\n",
      "[243 rows x 4 columns]\n",
      "\n",
      "=== distribution des jours de rapport principaux ===\n",
      "report_day\n",
      "Sunday       232\n",
      "Monday        10\n",
      "Wednesday      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== analyse de la regularite temporelle (exemple: France) ===\n",
      "nombre de rapports pour la France: 167\n",
      "intervalle moyen entre rapports: 7.5 jours\n",
      "ecart-type de l'intervalle: 6.0 jours\n",
      "\n",
      "distribution des intervalles entre rapports:\n",
      "days_since_last\n",
      "7.000     164\n",
      "14.000      1\n",
      "84.000      1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# analyse du pattern de rapport par pays\n",
    "print(\"=== analyse du pattern de rapport par pays ===\")\n",
    "\n",
    "# calculer le pourcentage de zeros par jour de la semaine pour chaque pays\n",
    "country_patterns = []\n",
    "for country in covid_data['location'].unique():\n",
    "    country_data = covid_data[covid_data['location'] == country].copy()\n",
    "    \n",
    "    # compter les zeros par jour de la semaine\n",
    "    pattern = country_data.groupby('day_of_week')['new_cases'].apply(lambda x: (x == 0).mean() * 100)\n",
    "    pattern = pattern.reindex(day_order)\n",
    "    \n",
    "    # identifier le jour de rapport principal (jour avec le moins de zeros)\n",
    "    report_day = pattern.idxmin()\n",
    "    min_zero_pct = pattern.min()\n",
    "    \n",
    "    country_patterns.append({\n",
    "        'country': country,\n",
    "        'report_day': report_day,\n",
    "        'report_day_zero_pct': min_zero_pct,\n",
    "        'autres_jours_zero_pct': pattern[pattern.index != report_day].mean()\n",
    "    })\n",
    "\n",
    "patterns_df = pd.DataFrame(country_patterns)\n",
    "print(\"\\npatterns de rapport pour 20 pays (echantillon):\")\n",
    "print(patterns_df)\n",
    "\n",
    "# distribution des jours de rapport\n",
    "print(\"\\n=== distribution des jours de rapport principaux ===\")\n",
    "report_day_dist = patterns_df['report_day'].value_counts()\n",
    "print(report_day_dist)\n",
    "\n",
    "# analyser la regularite temporelle des rapports pour un pays exemple\n",
    "print(\"\\n=== analyse de la regularite temporelle (exemple: France) ===\")\n",
    "france_data = covid_data[covid_data['location'] == 'France'].copy()\n",
    "france_data = france_data.sort_values('date')\n",
    "\n",
    "# identifier les jours avec des rapports\n",
    "france_reports = france_data[france_data['new_cases'] > 0][['date', 'new_cases', 'day_of_week']]\n",
    "france_reports['days_since_last'] = france_reports['date'].diff().dt.days\n",
    "\n",
    "print(f\"nombre de rapports pour la France: {len(france_reports)}\")\n",
    "print(f\"intervalle moyen entre rapports: {france_reports['days_since_last'].mean():.1f} jours\")\n",
    "print(f\"ecart-type de l'intervalle: {france_reports['days_since_last'].std():.1f} jours\")\n",
    "print(\"\\ndistribution des intervalles entre rapports:\")\n",
    "print(france_reports['days_since_last'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1e34059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== analyse de la completude des donnees hebdomadaires ===\n",
      "         country  total_weeks  weeks_with_reports  completeness_pct  \\\n",
      "0         France          243                 167            68.724   \n",
      "1        Germany          243                 181            74.486   \n",
      "2          Italy          244                 234            95.902   \n",
      "3          Spain          243                 181            74.486   \n",
      "4  United States          243                 173            71.193   \n",
      "\n",
      "   correlation_new_vs_total  \n",
      "0                     1.000  \n",
      "1                     0.999  \n",
      "2                     0.999  \n",
      "3                     1.000  \n",
      "4                     0.999  \n",
      "\n",
      "=== exemple detaille pour la France (10 premieres semaines avec donnees) ===\n",
      "          date_start   date_end  new_cases_sum  cases_diff  new_deaths_sum  \\\n",
      "year_week                                                                    \n",
      "2020-03   2020-01-20 2020-01-26          3.000       3.000           0.000   \n",
      "2020-04   2020-01-27 2020-02-02          3.000       3.000           0.000   \n",
      "2020-05   2020-02-03 2020-02-09          6.000       6.000           0.000   \n",
      "2020-07   2020-02-17 2020-02-23          4.000       4.000           0.000   \n",
      "2020-19   2020-05-11 2020-05-17        677.000     677.000         810.000   \n",
      "2020-20   2020-05-18 2020-05-24       4340.000    4340.000         562.000   \n",
      "2020-21   2020-05-25 2020-05-31       3436.000    3436.000         471.000   \n",
      "2020-22   2020-06-01 2020-06-07       2745.000    2745.000         331.000   \n",
      "2020-23   2020-06-08 2020-06-14       2627.000    2627.000         258.000   \n",
      "2020-24   2020-06-15 2020-06-21       2267.000    2267.000         159.000   \n",
      "\n",
      "           deaths_diff  \n",
      "year_week               \n",
      "2020-03          0.000  \n",
      "2020-04          0.000  \n",
      "2020-05          0.000  \n",
      "2020-07          0.000  \n",
      "2020-19        810.000  \n",
      "2020-20        562.000  \n",
      "2020-21        471.000  \n",
      "2020-22        331.000  \n",
      "2020-23        258.000  \n",
      "2020-24        159.000  \n",
      "\n",
      "=== verification de la coherence des donnees ===\n",
      "nombre de semaines avec incoherence entre new_cases et total_cases: 1\n",
      "exemples d'incoherences:\n",
      "          date_start  new_cases_sum  cases_diff\n",
      "year_week                                      \n",
      "2023-00   2023-01-01     151707.000       0.000\n"
     ]
    }
   ],
   "source": [
    "# analyse de la completude des donnees hebdomadaires\n",
    "print(\"=== analyse de la completude des donnees hebdomadaires ===\")\n",
    "\n",
    "# creer des agregations hebdomadaires\n",
    "covid_data['year_week'] = covid_data['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "# analyser la completude par semaine pour un echantillon de pays\n",
    "sample_countries = ['France', 'Germany', 'Italy', 'Spain', 'United States']\n",
    "completeness_analysis = []\n",
    "\n",
    "for country in sample_countries:\n",
    "    country_data = covid_data[covid_data['location'] == country]\n",
    "    \n",
    "    # nombre de semaines avec au moins un rapport\n",
    "    weeks_with_data = country_data[country_data['new_cases'] > 0].groupby('year_week').size()\n",
    "    total_weeks = country_data['year_week'].nunique()\n",
    "    weeks_with_reports = len(weeks_with_data)\n",
    "    \n",
    "    # verifier la coherence entre new_cases hebdomadaires et differences de total_cases\n",
    "    weekly_data = country_data.groupby('year_week').agg({\n",
    "        'new_cases': 'sum',\n",
    "        'total_cases': ['first', 'last'],\n",
    "        'date': ['min', 'max']\n",
    "    })\n",
    "    weekly_data.columns = ['new_cases_sum', 'total_cases_start', 'total_cases_end', 'date_start', 'date_end']\n",
    "    weekly_data['total_cases_diff'] = weekly_data['total_cases_end'] - weekly_data['total_cases_start']\n",
    "    \n",
    "    # calculer la correlation entre somme hebdomadaire et difference de total_cases\n",
    "    valid_weeks = weekly_data.dropna()\n",
    "    if len(valid_weeks) > 0:\n",
    "        correlation = valid_weeks['new_cases_sum'].corr(valid_weeks['total_cases_diff'])\n",
    "    else:\n",
    "        correlation = np.nan\n",
    "    \n",
    "    completeness_analysis.append({\n",
    "        'country': country,\n",
    "        'total_weeks': total_weeks,\n",
    "        'weeks_with_reports': weeks_with_reports,\n",
    "        'completeness_pct': weeks_with_reports / total_weeks * 100,\n",
    "        'correlation_new_vs_total': correlation\n",
    "    })\n",
    "\n",
    "completeness_df = pd.DataFrame(completeness_analysis)\n",
    "print(completeness_df)\n",
    "\n",
    "# exemple detaille pour la France\n",
    "print(\"\\n=== exemple detaille pour la France (10 premieres semaines avec donnees) ===\")\n",
    "france_weekly = covid_data[covid_data['location'] == 'France'].groupby('year_week').agg({\n",
    "    'new_cases': 'sum',\n",
    "    'new_deaths': 'sum',\n",
    "    'total_cases': ['first', 'last'],\n",
    "    'total_deaths': ['first', 'last'],\n",
    "    'date': ['min', 'max']\n",
    "})\n",
    "france_weekly.columns = ['new_cases_sum', 'new_deaths_sum', 'total_cases_start', 'total_cases_end', \n",
    "                        'total_deaths_start', 'total_deaths_end', 'date_start', 'date_end']\n",
    "france_weekly['cases_diff'] = france_weekly['total_cases_end'] - france_weekly['total_cases_start']\n",
    "france_weekly['deaths_diff'] = france_weekly['total_deaths_end'] - france_weekly['total_deaths_start']\n",
    "\n",
    "# afficher les premieres semaines avec des cas\n",
    "france_with_cases = france_weekly[france_weekly['new_cases_sum'] > 0].head(10)\n",
    "print(france_with_cases[['date_start', 'date_end', 'new_cases_sum', 'cases_diff', 'new_deaths_sum', 'deaths_diff']])\n",
    "\n",
    "# verifier la coherence\n",
    "print(\"\\n=== verification de la coherence des donnees ===\")\n",
    "discrepancies = france_weekly[(france_weekly['new_cases_sum'] != france_weekly['cases_diff']) & \n",
    "                              (france_weekly['new_cases_sum'] > 0)]\n",
    "print(f\"nombre de semaines avec incoherence entre new_cases et total_cases: {len(discrepancies)}\")\n",
    "if len(discrepancies) > 0:\n",
    "    print(\"exemples d'incoherences:\")\n",
    "    print(discrepancies[['date_start', 'new_cases_sum', 'cases_diff']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d709dfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== creation des donnees hebdomadaires agregees ===\n",
      "dimensions des donnees hebdomadaires: (57569, 23)\n",
      "nombre de semaines uniques: 242\n",
      "plage de semaines: 2020-01 a 2024-33\n",
      "\n",
      "=== qualite des donnees hebdomadaires ===\n",
      "nombre de lignes avec weekly_new_cases > 0: 38929\n",
      "pourcentage de donnees utiles: 67.62146294012402 %\n",
      "\n",
      "=== exemple de donnees hebdomadaires (France, 5 premieres semaines avec cas) ===\n",
      "      location year_week week_start  weekly_new_cases  weekly_new_deaths  \\\n",
      "17482   France   2020-04 2020-01-20             3.000              0.000   \n",
      "17483   France   2020-05 2020-01-27             3.000              0.000   \n",
      "17484   France   2020-06 2020-02-03             6.000              0.000   \n",
      "17486   France   2020-08 2020-02-17             4.000              0.000   \n",
      "17498   France   2020-20 2020-05-11           677.000            810.000   \n",
      "\n",
      "       avg_reproduction_rate  mortality_rate  avg_stringency_index  \n",
      "17482                    NaN           0.000                 3.177  \n",
      "17483                    NaN           0.000                 5.560  \n",
      "17484                    NaN           0.000                 5.560  \n",
      "17486                    NaN           0.000                 5.560  \n",
      "17498                  0.640         100.000                76.850  \n"
     ]
    }
   ],
   "source": [
    "# strategie d'agregation hebdomadaire\n",
    "print(\"=== creation des donnees hebdomadaires agregees ===\")\n",
    "\n",
    "def create_weekly_aggregates(df):\n",
    "    \"\"\"\n",
    "    agregation hebdomadaire des donnees covid avec calcul des metriques cibles\n",
    "    \"\"\"\n",
    "    # trier par location et date\n",
    "    df = df.sort_values(['location', 'date'])\n",
    "    \n",
    "    # creer l'identifiant de semaine iso\n",
    "    df['year_week'] = df['date'].dt.isocalendar().year.astype(str) + '-' + \\\n",
    "                      df['date'].dt.isocalendar().week.astype(str).str.zfill(2)\n",
    "    \n",
    "    # agregations hebdomadaires\n",
    "    weekly_agg = df.groupby(['location', 'year_week']).agg({\n",
    "        # dates\n",
    "        'date': ['min', 'max'],\n",
    "        \n",
    "        # cas et deces\n",
    "        'new_cases': 'sum',\n",
    "        'new_deaths': 'sum',\n",
    "        'total_cases': ['first', 'last'],\n",
    "        'total_deaths': ['first', 'last'],\n",
    "        \n",
    "        # donnees lissees\n",
    "        'new_cases_smoothed': 'mean',\n",
    "        'new_deaths_smoothed': 'mean',\n",
    "        \n",
    "        # metriques par million\n",
    "        'new_cases_per_million': 'mean',\n",
    "        'new_deaths_per_million': 'mean',\n",
    "        \n",
    "        # autres metriques\n",
    "        'reproduction_rate': 'mean',\n",
    "        'mortality_rate': 'last',\n",
    "        'stringency_index': 'mean',\n",
    "        \n",
    "        # donnees demographiques (constantes)\n",
    "        'population': 'first',\n",
    "        'population_density': 'first',\n",
    "        'continent': 'first',\n",
    "        'iso_code': 'first',\n",
    "        'latitude': 'first',\n",
    "        'longitude': 'first'\n",
    "    })\n",
    "    \n",
    "    # aplatir les noms de colonnes\n",
    "    weekly_agg.columns = ['_'.join(col).strip() for col in weekly_agg.columns.values]\n",
    "    weekly_agg = weekly_agg.reset_index()\n",
    "    \n",
    "    # renommer les colonnes pour plus de clarte\n",
    "    column_mapping = {\n",
    "        'date_min': 'week_start',\n",
    "        'date_max': 'week_end',\n",
    "        'new_cases_sum': 'weekly_new_cases',\n",
    "        'new_deaths_sum': 'weekly_new_deaths',\n",
    "        'total_cases_first': 'total_cases_start',\n",
    "        'total_cases_last': 'total_cases_end',\n",
    "        'total_deaths_first': 'total_deaths_start',\n",
    "        'total_deaths_last': 'total_deaths_end',\n",
    "        'new_cases_smoothed_mean': 'avg_daily_cases_smoothed',\n",
    "        'new_deaths_smoothed_mean': 'avg_daily_deaths_smoothed',\n",
    "        'new_cases_per_million_mean': 'avg_cases_per_million',\n",
    "        'new_deaths_per_million_mean': 'avg_deaths_per_million',\n",
    "        'reproduction_rate_mean': 'avg_reproduction_rate',\n",
    "        'mortality_rate_last': 'mortality_rate',\n",
    "        'stringency_index_mean': 'avg_stringency_index',\n",
    "        'population_first': 'population',\n",
    "        'population_density_first': 'population_density',\n",
    "        'continent_first': 'continent',\n",
    "        'iso_code_first': 'iso_code',\n",
    "        'latitude_first': 'latitude',\n",
    "        'longitude_first': 'longitude'\n",
    "    }\n",
    "    weekly_agg = weekly_agg.rename(columns=column_mapping)\n",
    "    \n",
    "    return weekly_agg\n",
    "\n",
    "# appliquer l'agregation\n",
    "covid_weekly = create_weekly_aggregates(covid_data)\n",
    "\n",
    "print(f\"dimensions des donnees hebdomadaires: {covid_weekly.shape}\")\n",
    "print(f\"nombre de semaines uniques: {covid_weekly['year_week'].nunique()}\")\n",
    "print(f\"plage de semaines: {covid_weekly['year_week'].min()} a {covid_weekly['year_week'].max()}\")\n",
    "\n",
    "# verification de la qualite\n",
    "print(\"\\n=== qualite des donnees hebdomadaires ===\")\n",
    "print(\"nombre de lignes avec weekly_new_cases > 0:\", (covid_weekly['weekly_new_cases'] > 0).sum())\n",
    "print(\"pourcentage de donnees utiles:\", (covid_weekly['weekly_new_cases'] > 0).sum() / len(covid_weekly) * 100, \"%\")\n",
    "\n",
    "# exemple de donnees hebdomadaires\n",
    "print(\"\\n=== exemple de donnees hebdomadaires (France, 5 premieres semaines avec cas) ===\")\n",
    "france_weekly = covid_weekly[(covid_weekly['location'] == 'France') & \n",
    "                            (covid_weekly['weekly_new_cases'] > 0)].head()\n",
    "cols_to_show = ['location', 'year_week', 'week_start', 'weekly_new_cases', 'weekly_new_deaths', \n",
    "                'avg_reproduction_rate', 'mortality_rate', 'avg_stringency_index']\n",
    "print(france_weekly[cols_to_show])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cef3cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification complete des donnees covid\n",
      "==================================================\n",
      "\n",
      "1. structure des donnees\n",
      "   dimensions: (421264, 25)\n",
      "   nombre de pays: 243\n",
      "   plage temporelle: 2020-01-01 00:00:00 a 2024-08-14 00:00:00\n",
      "   nombre de jours: 1688\n",
      "\n",
      "2. coherence temporelle\n",
      "   nombre de lignes attendues: 410184\n",
      "   nombre de lignes reelles: 421264\n",
      "   lignes manquantes: -11080\n",
      "\n",
      "   pays avec dates manquantes: 243\n",
      "   top 5 pays avec le plus de dates manquantes:\n",
      "        country  missing_count\n",
      " Western Sahara           1687\n",
      "Northern Cyprus            997\n",
      "          Macao            893\n",
      "          Wales            490\n",
      "       Scotland            383\n",
      "\n",
      "3. coherence des donnees cumulatives\n",
      "   pays avec problemes de coherence: 24\n",
      "   top 10 pays avec le plus de problemes:\n",
      "                         country  cases_decreases  deaths_decreases  new_vs_total_discrepancies  total_issues\n",
      "Saint Vincent and the Grenadines                0                 0                         290           290\n",
      "                          Russia                0                 0                         232           232\n",
      "                         Myanmar                0                 0                         228           228\n",
      "                         Bolivia                0                 0                         206           206\n",
      "                         Vietnam                0                 0                         190           190\n",
      "                     South Korea                0                 0                         190           190\n",
      "                          Brunei                0                 0                         184           184\n",
      "                   Cote d'Ivoire                0                 0                         183           183\n",
      "                       Venezuela                0                 0                         164           164\n",
      "                           Libya                0                 0                         159           159\n",
      "\n",
      "4. detection des valeurs aberrantes\n",
      "   new_cases negatifs: 0\n",
      "   new_deaths negatifs: 0\n",
      "   new_cases extremes (>3 std): 138 valeurs\n",
      "   top 5 new_cases:\n",
      "      China (2022-12-25): 40,475,477\n",
      "      China (2023-01-01): 24,644,876\n",
      "      China (2022-12-18): 11,098,550\n",
      "      China (2023-01-08): 8,061,762\n",
      "      United States (2022-01-16): 5,650,933\n",
      "   new_deaths extremes (>3 std): 401 valeurs\n",
      "   top 5 new_deaths:\n",
      "      China (2023-02-05): 47,687\n",
      "      India (2021-05-23): 28,982\n",
      "      India (2021-05-16): 27,922\n",
      "      India (2021-05-09): 26,820\n",
      "      India (2021-05-30): 26,706\n",
      "\n",
      "5. analyse des valeurs manquantes\n",
      "   colonnes avec valeurs manquantes:\n",
      "                         column  missing_count  missing_pct\n",
      "              reproduction_rate         225944       53.630\n",
      "               stringency_index         214165       50.840\n",
      "             population_density          45450       10.790\n",
      "                 mortality_rate          40872        9.700\n",
      "                       latitude          15969        3.790\n",
      "                      longitude          15969        3.790\n",
      " new_cases_smoothed_per_million          14004        3.320\n",
      "             new_cases_smoothed          14004        3.320\n",
      "            new_deaths_smoothed          13555        3.220\n",
      "new_deaths_smoothed_per_million          13555        3.220\n",
      "                      new_cases          12779        3.030\n",
      "          new_cases_per_million          12779        3.030\n",
      "                     new_deaths          12330        2.930\n",
      "         new_deaths_per_million          12330        2.930\n",
      "                    total_cases          11134        2.640\n",
      "                   total_deaths          11134        2.640\n",
      "       total_deaths_per_million          11134        2.640\n",
      "\n",
      "6. analyse des patterns de rapport\n",
      "   dimanche - cas > 0: 41040 (68.0%)\n",
      "   autres jours - cas > 0: 0 (0.0%)\n",
      "\n",
      "7. coherence des metadonnees\n",
      "\n",
      "==================================================\n",
      "resume de la verification\n",
      "==================================================\n",
      "donnees analysees: 421,264 lignes\n",
      "problemes detectes:\n",
      "  - dates manquantes: oui\n",
      "  - incoherences cumulatives: oui\n",
      "  - valeurs negatives: non\n",
      "  - metadonnees incoherentes: non\n",
      "  - pattern dimanche confirme: oui\n"
     ]
    }
   ],
   "source": [
    "# verification complete des donnees covid\n",
    "print(\"verification complete des donnees covid\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. verification des dimensions et structure\n",
    "print(\"\\n1. structure des donnees\")\n",
    "print(f\"   dimensions: {covid_data.shape}\")\n",
    "print(f\"   nombre de pays: {covid_data['location'].nunique()}\")\n",
    "print(f\"   plage temporelle: {covid_data['date'].min()} a {covid_data['date'].max()}\")\n",
    "print(f\"   nombre de jours: {(covid_data['date'].max() - covid_data['date'].min()).days + 1}\")\n",
    "\n",
    "# 2. verification de la coherence temporelle\n",
    "print(\"\\n2. coherence temporelle\")\n",
    "expected_rows = covid_data['location'].nunique() * ((covid_data['date'].max() - covid_data['date'].min()).days + 1)\n",
    "print(f\"   nombre de lignes attendues: {expected_rows}\")\n",
    "print(f\"   nombre de lignes reelles: {len(covid_data)}\")\n",
    "print(f\"   lignes manquantes: {expected_rows - len(covid_data)}\")\n",
    "\n",
    "# identifier les dates manquantes par pays\n",
    "missing_dates = []\n",
    "for country in covid_data['location'].unique():\n",
    "    country_data = covid_data[covid_data['location'] == country]\n",
    "    date_range = pd.date_range(start=covid_data['date'].min(), end=covid_data['date'].max())\n",
    "    existing_dates = set(country_data['date'])\n",
    "    missing = set(date_range) - existing_dates\n",
    "    if missing:\n",
    "        missing_dates.append({'country': country, 'missing_count': len(missing)})\n",
    "\n",
    "if missing_dates:\n",
    "    missing_df = pd.DataFrame(missing_dates).sort_values('missing_count', ascending=False)\n",
    "    print(f\"\\n   pays avec dates manquantes: {len(missing_df)}\")\n",
    "    print(\"   top 5 pays avec le plus de dates manquantes:\")\n",
    "    print(missing_df.head().to_string(index=False))\n",
    "\n",
    "# 3. verification de la coherence des donnees cumulatives\n",
    "print(\"\\n3. coherence des donnees cumulatives\")\n",
    "coherence_issues = []\n",
    "\n",
    "for country in covid_data['location'].unique():\n",
    "    country_data = covid_data[covid_data['location'] == country].sort_values('date')\n",
    "    \n",
    "    # verifier que total_cases est croissant\n",
    "    cases_decreases = (country_data['total_cases'].diff() < 0).sum()\n",
    "    deaths_decreases = (country_data['total_deaths'].diff() < 0).sum()\n",
    "    \n",
    "    # verifier la coherence entre new_cases et total_cases\n",
    "    country_data['calculated_new_cases'] = country_data['total_cases'].diff()\n",
    "    discrepancies = country_data[\n",
    "        (country_data['new_cases'].notna()) & \n",
    "        (country_data['calculated_new_cases'].notna()) &\n",
    "        (abs(country_data['new_cases'] - country_data['calculated_new_cases']) > 1)\n",
    "    ]\n",
    "    \n",
    "    if cases_decreases > 0 or deaths_decreases > 0 or len(discrepancies) > 0:\n",
    "        coherence_issues.append({\n",
    "            'country': country,\n",
    "            'cases_decreases': cases_decreases,\n",
    "            'deaths_decreases': deaths_decreases,\n",
    "            'new_vs_total_discrepancies': len(discrepancies)\n",
    "        })\n",
    "\n",
    "coherence_df = pd.DataFrame(coherence_issues)\n",
    "print(f\"   pays avec problemes de coherence: {len(coherence_df)}\")\n",
    "if len(coherence_df) > 0:\n",
    "    print(\"   top 10 pays avec le plus de problemes:\")\n",
    "    coherence_df['total_issues'] = coherence_df['cases_decreases'] + coherence_df['deaths_decreases'] + coherence_df['new_vs_total_discrepancies']\n",
    "    print(coherence_df.sort_values('total_issues', ascending=False).head(10).to_string(index=False))\n",
    "\n",
    "# 4. verification des valeurs aberrantes\n",
    "print(\"\\n4. detection des valeurs aberrantes\")\n",
    "\n",
    "# valeurs negatives\n",
    "negative_cases = (covid_data['new_cases'] < 0).sum()\n",
    "negative_deaths = (covid_data['new_deaths'] < 0).sum()\n",
    "print(f\"   new_cases negatifs: {negative_cases}\")\n",
    "print(f\"   new_deaths negatifs: {negative_deaths}\")\n",
    "\n",
    "# valeurs extremes (plus de 3 ecarts-types)\n",
    "for col in ['new_cases', 'new_deaths']:\n",
    "    if col in covid_data.columns:\n",
    "        data = covid_data[covid_data[col].notna() & (covid_data[col] > 0)][col]\n",
    "        if len(data) > 0:\n",
    "            mean = data.mean()\n",
    "            std = data.std()\n",
    "            threshold = mean + 3 * std\n",
    "            extremes = (covid_data[col] > threshold).sum()\n",
    "            print(f\"   {col} extremes (>3 std): {extremes} valeurs\")\n",
    "            \n",
    "            # afficher les 5 plus grandes valeurs\n",
    "            top_values = covid_data.nlargest(5, col)[['location', 'date', col]]\n",
    "            print(f\"   top 5 {col}:\")\n",
    "            for _, row in top_values.iterrows():\n",
    "                print(f\"      {row['location']} ({row['date'].strftime('%Y-%m-%d')}): {row[col]:,.0f}\")\n",
    "\n",
    "# 5. verification des taux de valeurs manquantes\n",
    "print(\"\\n5. analyse des valeurs manquantes\")\n",
    "missing_summary = []\n",
    "for col in covid_data.columns:\n",
    "    missing_count = covid_data[col].isna().sum()\n",
    "    missing_pct = missing_count / len(covid_data) * 100\n",
    "    if missing_pct > 0:\n",
    "        missing_summary.append({\n",
    "            'column': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': round(missing_pct, 2)\n",
    "        })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary).sort_values('missing_pct', ascending=False)\n",
    "print(\"   colonnes avec valeurs manquantes:\")\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# 6. verification des patterns de rapport hebdomadaire\n",
    "print(\"\\n6. analyse des patterns de rapport\")\n",
    "sunday_reporting = covid_data[covid_data['day_of_week'] == 'Sunday']['new_cases']\n",
    "other_days_reporting = covid_data[covid_data['day_of_week'] != 'Sunday']['new_cases']\n",
    "\n",
    "print(f\"   dimanche - cas > 0: {(sunday_reporting > 0).sum()} ({(sunday_reporting > 0).sum() / len(sunday_reporting) * 100:.1f}%)\")\n",
    "print(f\"   autres jours - cas > 0: {(other_days_reporting > 0).sum()} ({(other_days_reporting > 0).sum() / len(other_days_reporting) * 100:.1f}%)\")\n",
    "\n",
    "# 7. verification de la coherence des metadonnees\n",
    "print(\"\\n7. coherence des metadonnees\")\n",
    "metadata_issues = []\n",
    "\n",
    "for country in covid_data['location'].unique():\n",
    "    country_data = covid_data[covid_data['location'] == country]\n",
    "    \n",
    "    # verifier que les metadonnees sont constantes\n",
    "    unique_populations = country_data['population'].nunique()\n",
    "    unique_densities = country_data['population_density'].nunique()\n",
    "    unique_continents = country_data['continent'].nunique()\n",
    "    \n",
    "    if unique_populations > 1 or unique_densities > 1 or unique_continents > 1:\n",
    "        metadata_issues.append({\n",
    "            'country': country,\n",
    "            'unique_populations': unique_populations,\n",
    "            'unique_densities': unique_densities,\n",
    "            'unique_continents': unique_continents\n",
    "        })\n",
    "\n",
    "if metadata_issues:\n",
    "    print(f\"   pays avec metadonnees incoherentes: {len(metadata_issues)}\")\n",
    "    print(\"   exemples:\")\n",
    "    for issue in metadata_issues[:5]:\n",
    "        print(f\"      {issue}\")\n",
    "\n",
    "# resume final\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"resume de la verification\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"donnees analysees: {len(covid_data):,} lignes\")\n",
    "print(f\"problemes detectes:\")\n",
    "print(f\"  - dates manquantes: {'oui' if missing_dates else 'non'}\")\n",
    "print(f\"  - incoherences cumulatives: {'oui' if coherence_issues else 'non'}\")\n",
    "print(f\"  - valeurs negatives: {'oui' if negative_cases > 0 or negative_deaths > 0 else 'non'}\")\n",
    "print(f\"  - metadonnees incoherentes: {'oui' if metadata_issues else 'non'}\")\n",
    "print(f\"  - pattern dimanche confirme: oui\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "350f2b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correction et preparation des donnees\n",
      "==================================================\n",
      "\n",
      "1. filtrage des pays avec donnees insuffisantes\n",
      "   pays initiaux: 243\n",
      "   pays avec >= 100 points de donnees: 234\n",
      "   lignes apres filtrage: 410193 (97.4%)\n",
      "\n",
      "2. correction des incoherences new_cases vs total_cases\n",
      "   corrections effectuees: 2086\n",
      "\n",
      "3. gestion des valeurs extremes\n",
      "   seuil new_cases (99.9%): 1,430,714\n",
      "   seuil new_deaths (99.9%): 17,246\n",
      "   valeurs extremes marquees - cases: 39, deaths: 27\n",
      "\n",
      "4. preparation pour agregation hebdomadaire\n",
      "\n",
      "5. verification finale\n",
      "   donnees finales: 410,193 lignes\n",
      "   pays: 234\n",
      "   plage temporelle: 2020-01-01 00:00:00 a 2024-08-14 00:00:00\n",
      "   semaines uniques: 243\n",
      "   dimanches avec donnees: 38965 / 58806 (66.3%)\n",
      "\n",
      "   completude des colonnes essentielles:\n",
      "     new_cases: 99.6%\n",
      "     new_deaths: 99.7%\n",
      "     total_cases: 100.0%\n",
      "     total_deaths: 100.0%\n",
      "     population: 100.0%\n",
      "\n",
      "donnees pretes pour agregation hebdomadaire\n"
     ]
    }
   ],
   "source": [
    "# correction et preparation des donnees pour agregation hebdomadaire\n",
    "print(\"correction et preparation des donnees\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. filtrer les pays avec suffisamment de donnees\n",
    "print(\"\\n1. filtrage des pays avec donnees insuffisantes\")\n",
    "min_data_points = 100  # au moins 100 jours avec des donnees\n",
    "countries_data_count = covid_data.groupby('location')['new_cases'].count()\n",
    "valid_countries = countries_data_count[countries_data_count >= min_data_points].index.tolist()\n",
    "\n",
    "print(f\"   pays initiaux: {covid_data['location'].nunique()}\")\n",
    "print(f\"   pays avec >= {min_data_points} points de donnees: {len(valid_countries)}\")\n",
    "\n",
    "# filtrer le dataset\n",
    "covid_filtered = covid_data[covid_data['location'].isin(valid_countries)].copy()\n",
    "print(f\"   lignes apres filtrage: {len(covid_filtered)} ({len(covid_filtered)/len(covid_data)*100:.1f}%)\")\n",
    "\n",
    "# 2. corriger les incoherences entre new_cases et total_cases\n",
    "print(\"\\n2. correction des incoherences new_cases vs total_cases\")\n",
    "corrections_made = 0\n",
    "\n",
    "for country in covid_filtered['location'].unique():\n",
    "    country_idx = covid_filtered['location'] == country\n",
    "    country_data = covid_filtered[country_idx].sort_values('date')\n",
    "    \n",
    "    # calculer new_cases a partir de total_cases quand possible\n",
    "    calculated_new = country_data['total_cases'].diff()\n",
    "    \n",
    "    # remplacer new_cases par la difference de total_cases si incoherent\n",
    "    mask = (\n",
    "        country_data['new_cases'].notna() & \n",
    "        calculated_new.notna() & \n",
    "        (abs(country_data['new_cases'] - calculated_new) > 1)\n",
    "    )\n",
    "    \n",
    "    if mask.any():\n",
    "        covid_filtered.loc[country_data[mask].index, 'new_cases'] = calculated_new[mask]\n",
    "        corrections_made += mask.sum()\n",
    "\n",
    "print(f\"   corrections effectuees: {corrections_made}\")\n",
    "\n",
    "# 3. gerer les valeurs extremes\n",
    "print(\"\\n3. gestion des valeurs extremes\")\n",
    "# identifier les seuils raisonnables (99.9 percentile)\n",
    "threshold_cases = covid_filtered[covid_filtered['new_cases'] > 0]['new_cases'].quantile(0.999)\n",
    "threshold_deaths = covid_filtered[covid_filtered['new_deaths'] > 0]['new_deaths'].quantile(0.999)\n",
    "\n",
    "print(f\"   seuil new_cases (99.9%): {threshold_cases:,.0f}\")\n",
    "print(f\"   seuil new_deaths (99.9%): {threshold_deaths:,.0f}\")\n",
    "\n",
    "# marquer les valeurs extremes sans les supprimer\n",
    "covid_filtered['extreme_cases'] = covid_filtered['new_cases'] > threshold_cases\n",
    "covid_filtered['extreme_deaths'] = covid_filtered['new_deaths'] > threshold_deaths\n",
    "\n",
    "extreme_cases_count = covid_filtered['extreme_cases'].sum()\n",
    "extreme_deaths_count = covid_filtered['extreme_deaths'].sum()\n",
    "print(f\"   valeurs extremes marquees - cases: {extreme_cases_count}, deaths: {extreme_deaths_count}\")\n",
    "\n",
    "# 4. preparer l'agregation hebdomadaire\n",
    "print(\"\\n4. preparation pour agregation hebdomadaire\")\n",
    "\n",
    "# ajouter les informations temporelles necessaires\n",
    "covid_filtered['year'] = covid_filtered['date'].dt.year\n",
    "covid_filtered['week'] = covid_filtered['date'].dt.isocalendar().week\n",
    "covid_filtered['year_week'] = (\n",
    "    covid_filtered['year'].astype(str) + '-' + \n",
    "    covid_filtered['week'].astype(str).str.zfill(2)\n",
    ")\n",
    "\n",
    "# identifier le debut et fin de chaque semaine\n",
    "covid_filtered['week_start'] = covid_filtered['date'] - pd.to_timedelta(covid_filtered['date'].dt.dayofweek, unit='d')\n",
    "covid_filtered['week_end'] = covid_filtered['week_start'] + pd.Timedelta(days=6)\n",
    "\n",
    "# 5. verifier la qualite des donnees preparees\n",
    "print(\"\\n5. verification finale\")\n",
    "print(f\"   donnees finales: {len(covid_filtered):,} lignes\")\n",
    "print(f\"   pays: {covid_filtered['location'].nunique()}\")\n",
    "print(f\"   plage temporelle: {covid_filtered['date'].min()} a {covid_filtered['date'].max()}\")\n",
    "print(f\"   semaines uniques: {covid_filtered['year_week'].nunique()}\")\n",
    "\n",
    "# verifier qu'on a bien des donnees pour les dimanches\n",
    "sunday_data = covid_filtered[covid_filtered['date'].dt.dayofweek == 6]\n",
    "sunday_with_cases = sunday_data[sunday_data['new_cases'] > 0]\n",
    "print(f\"   dimanches avec donnees: {len(sunday_with_cases)} / {len(sunday_data)} ({len(sunday_with_cases)/len(sunday_data)*100:.1f}%)\")\n",
    "\n",
    "# statistiques sur les colonnes cles\n",
    "print(\"\\n   completude des colonnes essentielles:\")\n",
    "essential_cols = ['new_cases', 'new_deaths', 'total_cases', 'total_deaths', 'population']\n",
    "for col in essential_cols:\n",
    "    completeness = (covid_filtered[col].notna().sum() / len(covid_filtered) * 100)\n",
    "    print(f\"     {col}: {completeness:.1f}%\")\n",
    "\n",
    "print(\"\\ndonnees pretes pour agregation hebdomadaire\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ba51bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agregation hebdomadaire des donnees\n",
      "==================================================\n",
      "\n",
      "execution de l'agregation...\n",
      "\n",
      "calcul des differences hebdomadaires...\n",
      "\n",
      "==================================================\n",
      "resultats de l'agregation hebdomadaire\n",
      "==================================================\n",
      "nombre total de lignes: 56,874\n",
      "nombre de pays: 234\n",
      "nombre de semaines: 243\n",
      "plage temporelle: 2019-12-30 00:00:00 a 2024-08-18 00:00:00\n",
      "\n",
      "qualite des donnees hebdomadaires:\n",
      "  good: 56,130 (98.7%)\n",
      "  incomplete: 689 (1.2%)\n",
      "  extreme_values: 55 (0.1%)\n",
      "\n",
      "statistiques des cas hebdomadaires:\n",
      "  moyenne: 19,956\n",
      "  mediane: 395\n",
      "  max: 40,475,477\n",
      "\n",
      "verification de coherence (weekly_cases vs differences de totaux):\n",
      "  correlation: 1.000\n",
      "  ecart moyen: 0\n",
      "\n",
      "exemple de donnees agregees (france, 5 premieres semaines avec cas):\n",
      "location year_week  weekly_cases  weekly_deaths  avg_reproduction_rate  avg_stringency_index data_quality\n",
      "  France   2020-04         3.000          0.000                    NaN                 3.177         good\n",
      "  France   2020-05         3.000          0.000                    NaN                 5.560         good\n",
      "  France   2020-06         6.000          0.000                    NaN                 5.560         good\n",
      "  France   2020-08         4.000          0.000                    NaN                 5.560         good\n",
      "  France   2020-20       677.000        810.000                  0.640                76.850         good\n",
      "\n",
      "strategies de gestion des outliers:\n",
      "  valeurs cap (99.9%): cases <= 1,434,045, deaths <= 17,557\n",
      "  donnees marquees comme outliers: 14,837 (26.1%)\n",
      "\n",
      "  distribution des poids de regression:\n",
      "    poids 0.2: 14,837 lignes (26.1%)\n",
      "    poids 0.5: 500 lignes (0.9%)\n",
      "    poids 1.0: 41,537 lignes (73.0%)\n",
      "\n",
      "agregation hebdomadaire terminee avec succes\n"
     ]
    }
   ],
   "source": [
    "# agregation hebdomadaire des donnees covid\n",
    "print(\"agregation hebdomadaire des donnees\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# fonction d'agregation hebdomadaire\n",
    "def aggregate_weekly_data(df):\n",
    "    \"\"\"\n",
    "    agregation hebdomadaire avec calcul des metriques cles\n",
    "    \"\"\"\n",
    "    # grouper par pays et semaine\n",
    "    weekly_agg = df.groupby(['location', 'year_week', 'week_start', 'week_end']).agg({\n",
    "        # donnees cumulatives - prendre la derniere valeur de la semaine\n",
    "        'total_cases': 'last',\n",
    "        'total_deaths': 'last',\n",
    "        \n",
    "        # sommes hebdomadaires\n",
    "        'new_cases': 'sum',\n",
    "        'new_deaths': 'sum',\n",
    "        \n",
    "        # moyennes des donnees lissees\n",
    "        'new_cases_smoothed': 'mean',\n",
    "        'new_deaths_smoothed': 'mean',\n",
    "        \n",
    "        # moyennes des taux\n",
    "        'new_cases_per_million': 'mean',\n",
    "        'new_deaths_per_million': 'mean',\n",
    "        'total_deaths_per_million': 'last',\n",
    "        \n",
    "        # autres metriques - moyennes\n",
    "        'reproduction_rate': 'mean',\n",
    "        'mortality_rate': 'mean',\n",
    "        'stringency_index': 'mean',\n",
    "        \n",
    "        # metadonnees constantes - prendre la premiere valeur\n",
    "        'population': 'first',\n",
    "        'population_density': 'first',\n",
    "        'continent': 'first',\n",
    "        'iso_code': 'first',\n",
    "        'latitude': 'first',\n",
    "        'longitude': 'first',\n",
    "        \n",
    "        # marqueurs de qualite\n",
    "        'extreme_cases': 'any',\n",
    "        'extreme_deaths': 'any',\n",
    "        \n",
    "        # nombre de jours avec donnees dans la semaine\n",
    "        'date': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # renommer les colonnes\n",
    "    weekly_agg = weekly_agg.rename(columns={\n",
    "        'date': 'days_with_data',\n",
    "        'new_cases': 'weekly_cases',\n",
    "        'new_deaths': 'weekly_deaths',\n",
    "        'new_cases_smoothed': 'avg_daily_cases_smoothed',\n",
    "        'new_deaths_smoothed': 'avg_daily_deaths_smoothed',\n",
    "        'new_cases_per_million': 'avg_cases_per_million',\n",
    "        'new_deaths_per_million': 'avg_deaths_per_million',\n",
    "        'reproduction_rate': 'avg_reproduction_rate',\n",
    "        'mortality_rate': 'avg_mortality_rate',\n",
    "        'stringency_index': 'avg_stringency_index'\n",
    "    })\n",
    "    \n",
    "    return weekly_agg\n",
    "\n",
    "# executer l'agregation\n",
    "print(\"\\nexecution de l'agregation...\")\n",
    "covid_weekly = aggregate_weekly_data(covid_filtered)\n",
    "\n",
    "# calculer les differences hebdomadaires des totaux cumules\n",
    "print(\"\\ncalcul des differences hebdomadaires...\")\n",
    "covid_weekly = covid_weekly.sort_values(['location', 'week_start'])\n",
    "covid_weekly['weekly_cases_from_total'] = covid_weekly.groupby('location')['total_cases'].diff()\n",
    "covid_weekly['weekly_deaths_from_total'] = covid_weekly.groupby('location')['total_deaths'].diff()\n",
    "\n",
    "# calculer le taux de croissance hebdomadaire\n",
    "covid_weekly['cases_growth_rate'] = covid_weekly.groupby('location')['weekly_cases'].pct_change()\n",
    "covid_weekly['deaths_growth_rate'] = covid_weekly.groupby('location')['weekly_deaths'].pct_change()\n",
    "\n",
    "# ajouter des indicateurs de qualite et gestion des valeurs extremes\n",
    "covid_weekly['data_quality'] = 'good'\n",
    "covid_weekly.loc[covid_weekly['days_with_data'] < 3, 'data_quality'] = 'incomplete'\n",
    "covid_weekly.loc[covid_weekly['extreme_cases'] | covid_weekly['extreme_deaths'], 'data_quality'] = 'extreme_values'\n",
    "\n",
    "# creer des versions ajustees des donnees pour la regression\n",
    "# option 1: cap les valeurs extremes au 99.9 percentile\n",
    "cap_cases = covid_weekly[covid_weekly['weekly_cases'] > 0]['weekly_cases'].quantile(0.999)\n",
    "cap_deaths = covid_weekly[covid_weekly['weekly_deaths'] > 0]['weekly_deaths'].quantile(0.999)\n",
    "\n",
    "covid_weekly['weekly_cases_capped'] = covid_weekly['weekly_cases'].clip(upper=cap_cases)\n",
    "covid_weekly['weekly_deaths_capped'] = covid_weekly['weekly_deaths'].clip(upper=cap_deaths)\n",
    "\n",
    "# option 2: transformation logarithmique pour reduire l'impact des extremes\n",
    "covid_weekly['log_weekly_cases'] = np.log1p(covid_weekly['weekly_cases'])\n",
    "covid_weekly['log_weekly_deaths'] = np.log1p(covid_weekly['weekly_deaths'])\n",
    "\n",
    "# option 3: z-score pour identifier les outliers statistiques\n",
    "def calculate_robust_zscore(series):\n",
    "    \"\"\"calcul du z-score robuste base sur la mediane et mad\"\"\"\n",
    "    median = series.median()\n",
    "    mad = (series - median).abs().median()\n",
    "    if mad == 0:\n",
    "        return pd.Series(0, index=series.index)\n",
    "    return (series - median) / (1.4826 * mad)\n",
    "\n",
    "covid_weekly['zscore_cases'] = covid_weekly.groupby('location')['weekly_cases'].transform(calculate_robust_zscore)\n",
    "covid_weekly['zscore_deaths'] = covid_weekly.groupby('location')['weekly_deaths'].transform(calculate_robust_zscore)\n",
    "\n",
    "# marquer les outliers statistiques (zscore > 3)\n",
    "covid_weekly['is_outlier'] = (abs(covid_weekly['zscore_cases']) > 3) | (abs(covid_weekly['zscore_deaths']) > 3)\n",
    "\n",
    "# creer un poids pour la regression (downweight les outliers)\n",
    "covid_weekly['regression_weight'] = 1.0\n",
    "covid_weekly.loc[covid_weekly['data_quality'] == 'extreme_values', 'regression_weight'] = 0.1\n",
    "covid_weekly.loc[covid_weekly['data_quality'] == 'incomplete', 'regression_weight'] = 0.5\n",
    "covid_weekly.loc[covid_weekly['is_outlier'], 'regression_weight'] = 0.2\n",
    "\n",
    "# statistiques de l'agregation\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"resultats de l'agregation hebdomadaire\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"nombre total de lignes: {len(covid_weekly):,}\")\n",
    "print(f\"nombre de pays: {covid_weekly['location'].nunique()}\")\n",
    "print(f\"nombre de semaines: {covid_weekly['year_week'].nunique()}\")\n",
    "print(f\"plage temporelle: {covid_weekly['week_start'].min()} a {covid_weekly['week_end'].max()}\")\n",
    "\n",
    "# qualite des donnees\n",
    "print(\"\\nqualite des donnees hebdomadaires:\")\n",
    "quality_dist = covid_weekly['data_quality'].value_counts()\n",
    "for quality, count in quality_dist.items():\n",
    "    print(f\"  {quality}: {count:,} ({count/len(covid_weekly)*100:.1f}%)\")\n",
    "\n",
    "# statistiques des cas hebdomadaires\n",
    "print(\"\\nstatistiques des cas hebdomadaires:\")\n",
    "weekly_cases_stats = covid_weekly[covid_weekly['weekly_cases'] > 0]['weekly_cases'].describe()\n",
    "print(f\"  moyenne: {weekly_cases_stats['mean']:,.0f}\")\n",
    "print(f\"  mediane: {weekly_cases_stats['50%']:,.0f}\")\n",
    "print(f\"  max: {weekly_cases_stats['max']:,.0f}\")\n",
    "\n",
    "# verifier la coherence entre sommes et differences\n",
    "print(\"\\nverification de coherence (weekly_cases vs differences de totaux):\")\n",
    "mask = (covid_weekly['weekly_cases'].notna() & \n",
    "        covid_weekly['weekly_cases_from_total'].notna() & \n",
    "        (covid_weekly['weekly_cases'] > 0))\n",
    "coherent_data = covid_weekly[mask]\n",
    "if len(coherent_data) > 0:\n",
    "    correlation = coherent_data['weekly_cases'].corr(coherent_data['weekly_cases_from_total'])\n",
    "    print(f\"  correlation: {correlation:.3f}\")\n",
    "    \n",
    "    # calculer l'ecart moyen\n",
    "    coherent_data['difference'] = abs(coherent_data['weekly_cases'] - coherent_data['weekly_cases_from_total'])\n",
    "    avg_difference = coherent_data['difference'].mean()\n",
    "    print(f\"  ecart moyen: {avg_difference:,.0f}\")\n",
    "\n",
    "# exemple de donnees agregees\n",
    "print(\"\\nexemple de donnees agregees (france, 5 premieres semaines avec cas):\")\n",
    "france_sample = covid_weekly[\n",
    "    (covid_weekly['location'] == 'France') & \n",
    "    (covid_weekly['weekly_cases'] > 0)\n",
    "].head()\n",
    "display_cols = ['location', 'year_week', 'weekly_cases', 'weekly_deaths', \n",
    "                'avg_reproduction_rate', 'avg_stringency_index', 'data_quality']\n",
    "print(france_sample[display_cols].to_string(index=False))\n",
    "\n",
    "# afficher les strategies de gestion des outliers\n",
    "print(\"\\nstrategies de gestion des outliers:\")\n",
    "print(f\"  valeurs cap (99.9%): cases <= {cap_cases:,.0f}, deaths <= {cap_deaths:,.0f}\")\n",
    "print(f\"  donnees marquees comme outliers: {covid_weekly['is_outlier'].sum():,} ({covid_weekly['is_outlier'].sum()/len(covid_weekly)*100:.1f}%)\")\n",
    "print(\"\\n  distribution des poids de regression:\")\n",
    "weight_dist = covid_weekly['regression_weight'].value_counts().sort_index()\n",
    "for weight, count in weight_dist.items():\n",
    "    print(f\"    poids {weight}: {count:,} lignes ({count/len(covid_weekly)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nagregation hebdomadaire terminee avec succes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e720b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "verification finale avant sauvegarde\n",
      "==================================================\n",
      "\n",
      "1. analyse des outliers\n",
      "   nombre total d'outliers: 14,837\n",
      "   pays avec le plus d'outliers:\n",
      "     Mayotte: 117 outliers sur 243 semaines (48.1%)\n",
      "     El Salvador: 117 outliers sur 243 semaines (48.1%)\n",
      "     Egypt: 114 outliers sur 243 semaines (46.9%)\n",
      "     Palestine: 108 outliers sur 243 semaines (44.4%)\n",
      "     Algeria: 107 outliers sur 243 semaines (44.0%)\n",
      "     Guernsey: 107 outliers sur 243 semaines (44.0%)\n",
      "     Iraq: 106 outliers sur 243 semaines (43.6%)\n",
      "     Jordan: 106 outliers sur 243 semaines (43.6%)\n",
      "     Kazakhstan: 104 outliers sur 243 semaines (42.8%)\n",
      "     Bangladesh: 104 outliers sur 243 semaines (42.8%)\n",
      "\n",
      "2. distribution des z-scores\n",
      "   z-score cases - max: 24527.80\n",
      "   z-score cases - min: -1.03\n",
      "   z-score > 3: 13,238\n",
      "   z-score < -3: 0\n",
      "\n",
      "3. ajustement du critere d'outlier\n",
      "   outliers extremes (z > 5): 11,961 (21.0%)\n",
      "\n",
      "   distribution des poids ajustes:\n",
      "     poids 0.1: 2 lignes (0.0%)\n",
      "     poids 0.3: 11,961 lignes (21.0%)\n",
      "     poids 0.5: 526 lignes (0.9%)\n",
      "     poids 1.0: 44,385 lignes (78.0%)\n",
      "\n",
      "4. selection des colonnes pour la sauvegarde\n",
      "   toutes les 35 colonnes sont presentes\n",
      "\n",
      "5. sauvegarde terminee\n",
      "   fichier: /app/Model/ProcessedData2/covid_weekly_final.csv\n",
      "   taille: 56,874 lignes\n",
      "   pays: 234\n",
      "   semaines: 243\n",
      "\n",
      "6. echantillon des donnees sauvegardees (5 premieres lignes):\n",
      "   location year_week  weekly_cases  weekly_deaths  regression_weight_adjusted data_quality\n",
      "Afghanistan   2020-01         0.000          0.000                       0.500   incomplete\n",
      "Afghanistan   2020-02         0.000          0.000                       1.000         good\n",
      "Afghanistan   2020-03         0.000          0.000                       1.000         good\n",
      "Afghanistan   2020-04         0.000          0.000                       1.000         good\n",
      "Afghanistan   2020-05         0.000          0.000                       1.000         good\n",
      "\n",
      "==================================================\n",
      "donnees hebdomadaires nettoyees et sauvegardees avec succes!\n"
     ]
    }
   ],
   "source": [
    "# verification finale et sauvegarde des donnees\n",
    "print(\"verification finale avant sauvegarde\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. analyser pourquoi tant de donnees sont marquees comme outliers\n",
    "print(\"\\n1. analyse des outliers\")\n",
    "outliers = covid_weekly[covid_weekly['is_outlier']]\n",
    "print(f\"   nombre total d'outliers: {len(outliers):,}\")\n",
    "print(f\"   pays avec le plus d'outliers:\")\n",
    "outlier_by_country = outliers['location'].value_counts().head(10)\n",
    "for country, count in outlier_by_country.items():\n",
    "    total_weeks = len(covid_weekly[covid_weekly['location'] == country])\n",
    "    print(f\"     {country}: {count} outliers sur {total_weeks} semaines ({count/total_weeks*100:.1f}%)\")\n",
    "\n",
    "# 2. verifier la distribution des z-scores\n",
    "print(\"\\n2. distribution des z-scores\")\n",
    "print(f\"   z-score cases - max: {covid_weekly['zscore_cases'].max():.2f}\")\n",
    "print(f\"   z-score cases - min: {covid_weekly['zscore_cases'].min():.2f}\")\n",
    "print(f\"   z-score > 3: {(covid_weekly['zscore_cases'] > 3).sum():,}\")\n",
    "print(f\"   z-score < -3: {(covid_weekly['zscore_cases'] < -3).sum():,}\")\n",
    "\n",
    "# 3. ajuster le critere d'outlier pour etre moins strict\n",
    "print(\"\\n3. ajustement du critere d'outlier\")\n",
    "# utiliser un seuil plus eleve pour les outliers (z-score > 5 au lieu de 3)\n",
    "covid_weekly['is_extreme_outlier'] = (abs(covid_weekly['zscore_cases']) > 5) | (abs(covid_weekly['zscore_deaths']) > 5)\n",
    "\n",
    "# recalculer les poids avec le nouveau critere\n",
    "covid_weekly['regression_weight_adjusted'] = 1.0\n",
    "covid_weekly.loc[covid_weekly['data_quality'] == 'extreme_values', 'regression_weight_adjusted'] = 0.1\n",
    "covid_weekly.loc[covid_weekly['data_quality'] == 'incomplete', 'regression_weight_adjusted'] = 0.5\n",
    "covid_weekly.loc[covid_weekly['is_extreme_outlier'], 'regression_weight_adjusted'] = 0.3\n",
    "\n",
    "print(f\"   outliers extremes (z > 5): {covid_weekly['is_extreme_outlier'].sum():,} ({covid_weekly['is_extreme_outlier'].sum()/len(covid_weekly)*100:.1f}%)\")\n",
    "print(\"\\n   distribution des poids ajustes:\")\n",
    "weight_dist = covid_weekly['regression_weight_adjusted'].value_counts().sort_index()\n",
    "for weight, count in weight_dist.items():\n",
    "    print(f\"     poids {weight}: {count:,} lignes ({count/len(covid_weekly)*100:.1f}%)\")\n",
    "\n",
    "# 4. colonnes finales a sauvegarder\n",
    "print(\"\\n4. selection des colonnes pour la sauvegarde\")\n",
    "columns_to_save = [\n",
    "    # identifiants\n",
    "    'location', 'iso_code', 'continent',\n",
    "    'year_week', 'week_start', 'week_end',\n",
    "    \n",
    "    # donnees principales\n",
    "    'weekly_cases', 'weekly_deaths',\n",
    "    'total_cases', 'total_deaths',\n",
    "    \n",
    "    # donnees ajustees\n",
    "    'weekly_cases_capped', 'weekly_deaths_capped',\n",
    "    'log_weekly_cases', 'log_weekly_deaths',\n",
    "    \n",
    "    # moyennes hebdomadaires\n",
    "    'avg_daily_cases_smoothed', 'avg_daily_deaths_smoothed',\n",
    "    'avg_cases_per_million', 'avg_deaths_per_million',\n",
    "    'avg_reproduction_rate', 'avg_mortality_rate',\n",
    "    'avg_stringency_index',\n",
    "    \n",
    "    # taux de croissance\n",
    "    'cases_growth_rate', 'deaths_growth_rate',\n",
    "    \n",
    "    # donnees demographiques\n",
    "    'population', 'population_density',\n",
    "    'latitude', 'longitude',\n",
    "    \n",
    "    # indicateurs de qualite\n",
    "    'days_with_data', 'data_quality',\n",
    "    'is_outlier', 'is_extreme_outlier',\n",
    "    'regression_weight', 'regression_weight_adjusted',\n",
    "    'extreme_cases', 'extreme_deaths'\n",
    "]\n",
    "\n",
    "# verifier que toutes les colonnes existent\n",
    "missing_cols = [col for col in columns_to_save if col not in covid_weekly.columns]\n",
    "if missing_cols:\n",
    "    print(f\"   colonnes manquantes: {missing_cols}\")\n",
    "else:\n",
    "    print(f\"   toutes les {len(columns_to_save)} colonnes sont presentes\")\n",
    "\n",
    "# 5. sauvegarder les donnees\n",
    "covid_weekly_final = covid_weekly[columns_to_save].copy()\n",
    "\n",
    "# trier par pays et date\n",
    "covid_weekly_final = covid_weekly_final.sort_values(['location', 'week_start'])\n",
    "\n",
    "# sauvegarder\n",
    "output_file = PROC_DATA_DIR2 / \"covid_weekly_final.csv\"\n",
    "covid_weekly_final.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"\\n5. sauvegarde terminee\")\n",
    "print(f\"   fichier: {output_file}\")\n",
    "print(f\"   taille: {len(covid_weekly_final):,} lignes\")\n",
    "print(f\"   pays: {covid_weekly_final['location'].nunique()}\")\n",
    "print(f\"   semaines: {covid_weekly_final['year_week'].nunique()}\")\n",
    "\n",
    "# afficher un echantillon\n",
    "print(\"\\n6. echantillon des donnees sauvegardees (5 premieres lignes):\")\n",
    "sample_cols = ['location', 'year_week', 'weekly_cases', 'weekly_deaths', \n",
    "               'regression_weight_adjusted', 'data_quality']\n",
    "print(covid_weekly_final[sample_cols].head().to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"donnees hebdomadaires nettoyees et sauvegardees avec succes!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
