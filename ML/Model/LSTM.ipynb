{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf741c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 1 - Imports \n",
    "\n",
    "# imports pour manipulation de données\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# imports pour visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "# imports pour preprocessing et validation\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# imports pour deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "\n",
    "# imports pour analyse spatiale\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import pearsonr\n",
    "import networkx as nx\n",
    "\n",
    "# configuration gpu si disponible\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "    \n",
    "# seed pour reproductibilité\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# configuration des paramètres d'affichage\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "print(f\"gpu disponible: {len(physical_devices) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153906d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 2 - Chargement des données\n",
    "\n",
    "# chargement du dataset\n",
    "df = pd.read_csv('./ProcessedData2/covid_weekly_final.csv')\n",
    "\n",
    "# informations générales\n",
    "print(f\"shape du dataset: {df.shape}\")\n",
    "print(f\"periode couverte: {df['week_start'].min()} à {df['week_end'].max()}\")\n",
    "print(f\"nombre de pays/territoires: {df['location'].nunique()}\")\n",
    "print(f\"nombre de continents: {df['continent'].nunique()}\")\n",
    "print(f\"\\ntypes de données:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# verification des valeurs manquantes\n",
    "print(f\"\\n\\nvaleurs manquantes par colonne:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_percent = (missing_counts / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'count': missing_counts[missing_counts > 0],\n",
    "    'percent': missing_percent[missing_counts > 0]\n",
    "}).sort_values('percent', ascending=False)\n",
    "print(missing_df)\n",
    "\n",
    "# apercu des donnees\n",
    "print(f\"\\n\\npremières lignes:\")\n",
    "print(df.head())\n",
    "\n",
    "# statistiques sur les colonnes clés pour la modélisation\n",
    "key_cols = ['weekly_cases', 'weekly_deaths', 'weekly_cases_capped', 'weekly_deaths_capped',\n",
    "            'log_weekly_cases', 'log_weekly_deaths', 'avg_reproduction_rate', \n",
    "            'avg_mortality_rate', 'avg_stringency_index']\n",
    "\n",
    "print(f\"\\n\\nstatistiques des colonnes clés:\")\n",
    "print(df[key_cols].describe())\n",
    "\n",
    "# distribution des poids de regression\n",
    "print(f\"\\n\\ndistribution des poids de regression:\")\n",
    "print(df['regression_weight_adjusted'].value_counts().sort_index())\n",
    "\n",
    "# proportion des outliers\n",
    "print(f\"\\n\\nproportion des outliers:\")\n",
    "print(f\"outliers extremes: {df['is_extreme_outlier'].sum() / len(df) * 100:.2f}%\")\n",
    "print(f\"outliers standards: {df['is_outlier'].sum() / len(df) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2839ff7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 3 - Feature Engineering\n",
    "\n",
    "\n",
    "# conversion des dates\n",
    "df['week_start'] = pd.to_datetime(df['week_start'])\n",
    "df['week_end'] = pd.to_datetime(df['week_end'])\n",
    "\n",
    "# extraction des composantes temporelles\n",
    "df['year'] = df['week_start'].dt.year\n",
    "df['week_of_year'] = df['week_start'].dt.isocalendar().week\n",
    "df['month'] = df['week_start'].dt.month\n",
    "df['quarter'] = df['week_start'].dt.quarter\n",
    "\n",
    "# calcul du nombre de semaines depuis le début de la pandémie\n",
    "pandemic_start = df['week_start'].min()\n",
    "df['weeks_since_start'] = (df['week_start'] - pandemic_start).dt.days / 7\n",
    "\n",
    "# encodage cyclique pour capturer la saisonnalité\n",
    "df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# création de features pour les phases épidémiques\n",
    "# phase approximative basée sur le temps écoulé\n",
    "df['phase_early'] = (df['weeks_since_start'] < 12).astype(int)  # 3 premiers mois\n",
    "df['phase_peak1'] = ((df['weeks_since_start'] >= 12) & (df['weeks_since_start'] < 26)).astype(int)  # mois 3-6\n",
    "df['phase_summer2020'] = ((df['weeks_since_start'] >= 26) & (df['weeks_since_start'] < 39)).astype(int)  # été 2020\n",
    "df['phase_wave2'] = ((df['weeks_since_start'] >= 39) & (df['weeks_since_start'] < 65)).astype(int)  # 2ème vague\n",
    "df['phase_vaccination'] = (df['weeks_since_start'] >= 52).astype(int)  # après 1 an\n",
    "\n",
    "# calcul de la matrice de distances géographiques entre pays\n",
    "locations_with_coords = df[['location', 'latitude', 'longitude']].dropna().drop_duplicates()\n",
    "print(f\"nombre de pays avec coordonnées complètes: {len(locations_with_coords)}\")\n",
    "\n",
    "# creation d'un dictionnaire de coordonnées\n",
    "coords_dict = {}\n",
    "for _, row in locations_with_coords.iterrows():\n",
    "    coords_dict[row['location']] = (row['latitude'], row['longitude'])\n",
    "\n",
    "# calcul des distances entre tous les pays (en km)\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    r = 6371  # rayon de la terre en km\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    return r * c\n",
    "\n",
    "# creation d'une matrice de distances\n",
    "countries = list(coords_dict.keys())\n",
    "n_countries = len(countries)\n",
    "distance_matrix = np.zeros((n_countries, n_countries))\n",
    "\n",
    "for i, country1 in enumerate(countries):\n",
    "    for j, country2 in enumerate(countries):\n",
    "        if i != j:\n",
    "            lat1, lon1 = coords_dict[country1]\n",
    "            lat2, lon2 = coords_dict[country2]\n",
    "            distance_matrix[i, j] = haversine_distance(lat1, lon1, lat2, lon2)\n",
    "\n",
    "# sauvegarde pour usage ultérieur\n",
    "distance_df = pd.DataFrame(distance_matrix, index=countries, columns=countries)\n",
    "print(f\"\\nmatrice de distances créée: {distance_df.shape}\")\n",
    "print(f\"distance moyenne entre pays: {distance_matrix[distance_matrix > 0].mean():.0f} km\")\n",
    "\n",
    "# ajout de features géographiques au dataset\n",
    "# nombre de pays voisins dans un rayon de 1000km\n",
    "neighbor_counts = {}\n",
    "for country in countries:\n",
    "    if country in distance_df.index:\n",
    "        neighbors = (distance_df.loc[country] < 1000) & (distance_df.loc[country] > 0)\n",
    "        neighbor_counts[country] = neighbors.sum()\n",
    "\n",
    "df['neighbor_count_1000km'] = df['location'].map(neighbor_counts).fillna(0)\n",
    "\n",
    "# indicateur de connectivité continentale\n",
    "continent_sizes = df.groupby('continent')['location'].nunique()\n",
    "df['continent_connectivity'] = df['continent'].map(continent_sizes)\n",
    "\n",
    "print(f\"\\n\\nnouvelles colonnes créées:\")\n",
    "new_cols = [col for col in df.columns if col not in ['location', 'iso_code', 'continent', 'year_week', \n",
    "                                                       'week_start', 'week_end', 'weekly_cases', 'weekly_deaths',\n",
    "                                                       'total_cases', 'total_deaths', 'weekly_cases_capped', \n",
    "                                                       'weekly_deaths_capped', 'log_weekly_cases', 'log_weekly_deaths',\n",
    "                                                       'avg_daily_cases_smoothed', 'avg_daily_deaths_smoothed',\n",
    "                                                       'avg_cases_per_million', 'avg_deaths_per_million',\n",
    "                                                       'avg_reproduction_rate', 'avg_mortality_rate', 'avg_stringency_index',\n",
    "                                                       'cases_growth_rate', 'deaths_growth_rate', 'population',\n",
    "                                                       'population_density', 'latitude', 'longitude', 'days_with_data',\n",
    "                                                       'data_quality', 'is_outlier', 'is_extreme_outlier',\n",
    "                                                       'regression_weight', 'regression_weight_adjusted',\n",
    "                                                       'extreme_cases', 'extreme_deaths']]\n",
    "print(new_cols)\n",
    "\n",
    "# verification des features temporelles\n",
    "print(f\"\\n\\nverification des encodages cycliques:\")\n",
    "print(f\"week_sin range: [{df['week_sin'].min():.3f}, {df['week_sin'].max():.3f}]\")\n",
    "print(f\"week_cos range: [{df['week_cos'].min():.3f}, {df['week_cos'].max():.3f}]\")\n",
    "print(f\"weeks_since_start range: [{df['weeks_since_start'].min():.1f}, {df['weeks_since_start'].max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c90b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 4 - Préparation des données\n",
    "\n",
    "\n",
    "# stratégie de traitement des valeurs manquantes\n",
    "# on va créer des indicateurs de missingness avant imputation\n",
    "missing_indicators = ['avg_reproduction_rate', 'deaths_growth_rate', 'avg_stringency_index', \n",
    "                     'cases_growth_rate', 'avg_mortality_rate']\n",
    "\n",
    "for col in missing_indicators:\n",
    "    df[f'{col}_was_missing'] = df[col].isna().astype(int)\n",
    "\n",
    "# imputation des valeurs manquantes\n",
    "# pour les coordonnées géographiques, on utilise la médiane par continent\n",
    "for continent in df['continent'].unique():\n",
    "    if pd.notna(continent):\n",
    "        mask = df['continent'] == continent\n",
    "        df.loc[mask, 'latitude'] = df.loc[mask, 'latitude'].fillna(df.loc[mask, 'latitude'].median())\n",
    "        df.loc[mask, 'longitude'] = df.loc[mask, 'longitude'].fillna(df.loc[mask, 'longitude'].median())\n",
    "        df.loc[mask, 'population_density'] = df.loc[mask, 'population_density'].fillna(df.loc[mask, 'population_density'].median())\n",
    "\n",
    "# pour les métriques épidémiques, on fait un forward fill par pays puis backward fill\n",
    "epidemio_cols = ['avg_reproduction_rate', 'avg_mortality_rate', 'avg_stringency_index',\n",
    "                 'cases_growth_rate', 'deaths_growth_rate', 'avg_daily_cases_smoothed',\n",
    "                 'avg_daily_deaths_smoothed', 'avg_cases_per_million', 'avg_deaths_per_million']\n",
    "\n",
    "df_sorted = df.sort_values(['location', 'week_start'])\n",
    "for col in epidemio_cols:\n",
    "    df_sorted[col] = df_sorted.groupby('location')[col].fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "# pour les valeurs toujours manquantes, on utilise 0 ou la médiane globale\n",
    "df_sorted['avg_reproduction_rate'] = df_sorted['avg_reproduction_rate'].fillna(1.0)  # r0 neutre\n",
    "df_sorted['avg_mortality_rate'] = df_sorted['avg_mortality_rate'].fillna(0.0)\n",
    "df_sorted['avg_stringency_index'] = df_sorted['avg_stringency_index'].fillna(0.0)\n",
    "df_sorted['cases_growth_rate'] = df_sorted['cases_growth_rate'].fillna(0.0)\n",
    "df_sorted['deaths_growth_rate'] = df_sorted['deaths_growth_rate'].fillna(0.0)\n",
    "\n",
    "# remplissage des dernières valeurs manquantes\n",
    "for col in df_sorted.columns:\n",
    "    if df_sorted[col].dtype in ['float64', 'int64'] and df_sorted[col].isna().any():\n",
    "        df_sorted[col] = df_sorted[col].fillna(0)\n",
    "\n",
    "df = df_sorted.copy()\n",
    "\n",
    "# TARGETS\n",
    "\n",
    "# 1. MORTALITY RATE\n",
    "# Utilisation du ratio log pour capturer plus de variance\n",
    "df['log_cases'] = np.log1p(df['weekly_cases'])\n",
    "df['log_deaths'] = np.log1p(df['weekly_deaths'])\n",
    "df['target_mortality_rate'] = (df['log_deaths'] - df['log_cases']).clip(-5, 0)\n",
    "df['target_mortality_rate'] = (df['target_mortality_rate'] + 5) / 5\n",
    "\n",
    "# 2. TRANSMISSION RATE - Changements relatifs\n",
    "# Capture les variations plutôt que les valeurs absolues\n",
    "df['ma_reproduction'] = df.groupby('location')['avg_reproduction_rate'].transform(\n",
    "    lambda x: x.rolling(4, min_periods=1).mean()\n",
    ")\n",
    "df['target_transmission_rate'] = (df['avg_reproduction_rate'] - df['ma_reproduction']).clip(-2, 2)\n",
    "df['target_transmission_rate'] = (df['target_transmission_rate'] + 2) / 4\n",
    "\n",
    "# 3. SPATIAL SPREAD - Version simplifiée basée sur la croissance locale vs régionale\n",
    "df['regional_growth'] = df.groupby(['continent', 'week_start'])['cases_growth_rate'].transform('mean')\n",
    "df['target_spatial_spread'] = (df['cases_growth_rate'] - df['regional_growth']).clip(-1, 1)\n",
    "df['target_spatial_spread'] = (df['target_spatial_spread'] + 1) / 2\n",
    "\n",
    "# Vérification de la variance\n",
    "print(\"NOUVELLES statistiques des variables cibles:\")\n",
    "target_cols = ['target_mortality_rate', 'target_transmission_rate', 'target_spatial_spread']\n",
    "for col in target_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  mean: {df[col].mean():.4f}\")\n",
    "    print(f\"  std: {df[col].std():.4f}\")\n",
    "    print(f\"  min: {df[col].min():.4f}\")\n",
    "    print(f\"  max: {df[col].max():.4f}\")\n",
    "    print(f\"  variance: {df[col].var():.6f}\")\n",
    "\n",
    "# Visualisation rapide\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "for i, col in enumerate(target_cols):\n",
    "    axes[i].hist(df[col].dropna(), bins=50, alpha=0.7)\n",
    "    axes[i].set_title(f'{col} - Distribution')\n",
    "    axes[i].axvline(df[col].mean(), color='red', linestyle='--', label='mean')\n",
    "    axes[i].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n\\ncorrélations entre les targets:\")\n",
    "print(df[target_cols].corr())\n",
    "\n",
    "print(f\"\\n\\nvaleurs manquantes finales:\")\n",
    "print(df.isna().sum()[df.isna().sum() > 0])\n",
    "\n",
    "# sauvegarde du dataframe préparé\n",
    "df.to_pickle('covid_data_prepared.pkl')\n",
    "print(f\"\\n\\ndataset sauvegardé: covid_data_prepared.pkl\")\n",
    "print(f\"shape finale: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b395f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 5 - Création des séquences\n",
    "\n",
    "\n",
    "# chargement des données préparées\n",
    "df = pd.read_pickle('covid_data_prepared.pkl')\n",
    "\n",
    "# fonction pour détecter les phases épidémiques dynamiquement\n",
    "def detect_epidemic_phases(country_data):\n",
    "    \"\"\"\n",
    "    détecte les phases épidémiques basées sur les indicateurs dynamiques\n",
    "    plutôt que sur des dates fixes\n",
    "    \"\"\"\n",
    "    phases = np.zeros(len(country_data))\n",
    "    \n",
    "    # calcul des indicateurs sur fenêtre glissante\n",
    "    window = 3  # fenêtre de 3 semaines pour lisser\n",
    "    \n",
    "    # moving averages\n",
    "    ma_cases = country_data['log_weekly_cases'].rolling(window, center=True, min_periods=1).mean()\n",
    "    ma_r0 = country_data['avg_reproduction_rate'].rolling(window, center=True, min_periods=1).mean()\n",
    "    ma_growth = country_data['cases_growth_rate'].rolling(window, center=True, min_periods=1).mean()\n",
    "    \n",
    "    # détection des phases\n",
    "    for i in range(len(country_data)):\n",
    "        # phase 0: pre-épidémique (très peu de cas)\n",
    "        if ma_cases.iloc[i] < 1.0:  # log(e) = 1, donc moins de ~3 cas\n",
    "            phases[i] = 0\n",
    "        # phase 1: croissance initiale (r0 > 1.5 et croissance positive)\n",
    "        elif ma_r0.iloc[i] > 1.5 and ma_growth.iloc[i] > 0.1:\n",
    "            phases[i] = 1\n",
    "        # phase 2: pic (r0 proche de 1, croissance ralentit)\n",
    "        elif 0.8 <= ma_r0.iloc[i] <= 1.2 and abs(ma_growth.iloc[i]) < 0.1:\n",
    "            phases[i] = 2\n",
    "        # phase 3: déclin (r0 < 0.8 ou croissance négative forte)\n",
    "        elif ma_r0.iloc[i] < 0.8 or ma_growth.iloc[i] < -0.1:\n",
    "            phases[i] = 3\n",
    "        # phase 4: endémique/contrôlé (cas stables à niveau bas)\n",
    "        elif ma_cases.iloc[i] < 3.0 and abs(ma_growth.iloc[i]) < 0.05:\n",
    "            phases[i] = 4\n",
    "        # phase 5: résurgence (nouvelle croissance après déclin)\n",
    "        elif i > 20 and phases[i-10:i].mean() > 2 and ma_growth.iloc[i] > 0.1:\n",
    "            phases[i] = 5\n",
    "        else:\n",
    "            # par défaut, on propage la phase précédente\n",
    "            phases[i] = phases[i-1] if i > 0 else 1\n",
    "    \n",
    "    return phases\n",
    "\n",
    "# application de la détection de phases par pays\n",
    "print(\"détection des phases épidémiques dynamiques...\")\n",
    "df['epidemic_phase'] = 0\n",
    "\n",
    "for country in df['location'].unique():\n",
    "    mask = df['location'] == country\n",
    "    country_data = df[mask].sort_values('week_start').reset_index(drop=True)\n",
    "    \n",
    "    if len(country_data) > 10:  # suffisamment de données\n",
    "        phases = detect_epidemic_phases(country_data)\n",
    "        df.loc[mask, 'epidemic_phase'] = phases\n",
    "\n",
    "# création des features de phase (one-hot encoding)\n",
    "phase_names = ['pre_epidemic', 'growth', 'peak', 'decline', 'controlled', 'resurgence']\n",
    "for i, phase_name in enumerate(phase_names):\n",
    "    df[f'phase_{phase_name}'] = (df['epidemic_phase'] == i).astype(int)\n",
    "\n",
    "print(f\"distribution des phases épidémiques:\")\n",
    "phase_counts = df['epidemic_phase'].value_counts().sort_index()\n",
    "for i, count in phase_counts.items():\n",
    "    if i < len(phase_names):\n",
    "        print(f\"phase {i} ({phase_names[i]}): {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# sélection des features pour le modèle\n",
    "feature_cols = [\n",
    "    # variables épidémiologiques (transformées pour stabilité)\n",
    "    'log_weekly_cases', 'log_weekly_deaths',\n",
    "    'avg_cases_per_million', 'avg_deaths_per_million',\n",
    "    'avg_reproduction_rate', 'avg_mortality_rate',\n",
    "    'cases_growth_rate', 'deaths_growth_rate',\n",
    "    \n",
    "    # mesures de contrôle\n",
    "    'avg_stringency_index',\n",
    "    \n",
    "    # features temporelles (génériques, pas spécifiques au covid)\n",
    "    'weeks_since_start', 'week_sin', 'week_cos', 'month_sin', 'month_cos',\n",
    "    \n",
    "    # phases épidémiques dynamiques\n",
    "    'phase_pre_epidemic', 'phase_growth', 'phase_peak', \n",
    "    'phase_decline', 'phase_controlled', 'phase_resurgence',\n",
    "    \n",
    "    # features géographiques\n",
    "    'population_density', 'neighbor_count_1000km', 'continent_connectivity',\n",
    "    \n",
    "    # indicateurs de qualité des données\n",
    "    'regression_weight_adjusted',\n",
    "    'avg_reproduction_rate_was_missing', 'deaths_growth_rate_was_missing',\n",
    "    'avg_stringency_index_was_missing', 'cases_growth_rate_was_missing',\n",
    "    'avg_mortality_rate_was_missing'\n",
    "]\n",
    "\n",
    "target_cols = ['target_mortality_rate', 'target_transmission_rate', 'target_spatial_spread']\n",
    "\n",
    "# vérification et traitement des valeurs problématiques\n",
    "print(\"\\nvérification des valeurs problématiques dans les features:\")\n",
    "for col in feature_cols:\n",
    "    if col in df.columns:\n",
    "        inf_count = np.isinf(df[col]).sum()\n",
    "        nan_count = df[col].isna().sum()\n",
    "        if inf_count > 0 or nan_count > 0:\n",
    "            print(f\"{col}: {inf_count} inf, {nan_count} nan\")\n",
    "            # remplacement des inf par des valeurs extremes mais finies\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], [df[col][np.isfinite(df[col])].max(), \n",
    "                                                           df[col][np.isfinite(df[col])].min()])\n",
    "            # remplacement des nan restants par 0\n",
    "            df[col] = df[col].fillna(0)\n",
    "\n",
    "# capping des growth rates extremes\n",
    "growth_rate_cols = ['cases_growth_rate', 'deaths_growth_rate']\n",
    "for col in growth_rate_cols:\n",
    "    df[col] = df[col].clip(-10, 10)\n",
    "\n",
    "# normalisation robuste des features\n",
    "scaler_features = RobustScaler(quantile_range=(5, 95))\n",
    "scaler_targets = StandardScaler()  # CHANGÉ de MinMaxScaler\n",
    "\n",
    "# fit sur les données avec poids élevé uniquement\n",
    "high_quality_mask = df['regression_weight_adjusted'] >= 0.5\n",
    "features_scaled = scaler_features.fit_transform(df.loc[high_quality_mask, feature_cols])\n",
    "\n",
    "# transform sur toutes les données\n",
    "df_features_scaled = pd.DataFrame(\n",
    "    scaler_features.transform(df[feature_cols]),\n",
    "    columns=feature_cols,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "df_targets_scaled = pd.DataFrame(\n",
    "    scaler_targets.fit_transform(df[target_cols]),\n",
    "    columns=target_cols,\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# fonction pour créer des séquences\n",
    "def create_sequences(country_data, sequence_length=12, prediction_horizon=4):\n",
    "    \"\"\"\n",
    "    crée des séquences pour lstm\n",
    "    sequence_length: nombre de semaines historiques\n",
    "    prediction_horizon: nombre de semaines à prédire\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    weights = []\n",
    "    metadata = []\n",
    "    \n",
    "    n_samples = len(country_data) - sequence_length - prediction_horizon + 1\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # sequence historique\n",
    "        seq_features = country_data[feature_cols].iloc[i:i+sequence_length].values\n",
    "        \n",
    "        # targets futures (moyenne sur l'horizon de prédiction)\n",
    "        future_targets = country_data[target_cols].iloc[i+sequence_length:i+sequence_length+prediction_horizon].mean().values\n",
    "        \n",
    "        # poids de régression (minimum sur la séquence)\n",
    "        seq_weights = country_data['regression_weight_adjusted'].iloc[i:i+sequence_length+prediction_horizon].min()\n",
    "        \n",
    "        # metadata incluant la phase dominante\n",
    "        dominant_phase = country_data['epidemic_phase'].iloc[i+sequence_length:i+sequence_length+prediction_horizon].mode()[0]\n",
    "        meta = {\n",
    "            'location': country_data['location'].iloc[i],\n",
    "            'week_start': country_data['week_start'].iloc[i+sequence_length],\n",
    "            'epidemic_phase': dominant_phase,\n",
    "            'weeks_since_start': country_data['weeks_since_start'].iloc[i+sequence_length]\n",
    "        }\n",
    "        \n",
    "        sequences.append(seq_features)\n",
    "        targets.append(future_targets)\n",
    "        weights.append(seq_weights)\n",
    "        metadata.append(meta)\n",
    "    \n",
    "    return sequences, targets, weights, metadata\n",
    "\n",
    "# création des séquences par pays\n",
    "all_sequences = []\n",
    "all_targets = []\n",
    "all_weights = []\n",
    "all_metadata = []\n",
    "\n",
    "sequence_length = 12  # 3 mois d'historique\n",
    "prediction_horizon = 4  # prédiction sur 1 mois\n",
    "\n",
    "countries = df['location'].unique()\n",
    "print(f\"\\ncréation des séquences pour {len(countries)} pays...\")\n",
    "\n",
    "for country in countries:\n",
    "    country_data = df[df['location'] == country].sort_values('week_start').reset_index(drop=True)\n",
    "    \n",
    "    # on ne prend que les pays avec suffisamment de données\n",
    "    if len(country_data) >= sequence_length + prediction_horizon:\n",
    "        # merge avec les features et targets normalisés\n",
    "        country_features = df_features_scaled[df_features_scaled.index.isin(country_data.index)]\n",
    "        country_targets = df_targets_scaled[df_targets_scaled.index.isin(country_data.index)]\n",
    "        \n",
    "        country_data_scaled = country_data.copy()\n",
    "        country_data_scaled[feature_cols] = country_features.values\n",
    "        country_data_scaled[target_cols] = country_targets.values\n",
    "        \n",
    "        sequences, targets, weights, metadata = create_sequences(\n",
    "            country_data_scaled, \n",
    "            sequence_length, \n",
    "            prediction_horizon\n",
    "        )\n",
    "        \n",
    "        all_sequences.extend(sequences)\n",
    "        all_targets.extend(targets)\n",
    "        all_weights.extend(weights)\n",
    "        all_metadata.extend(metadata)\n",
    "\n",
    "# conversion en arrays numpy\n",
    "x = np.array(all_sequences)\n",
    "y = np.array(all_targets)\n",
    "sample_weights = np.array(all_weights)\n",
    "\n",
    "print(f\"\\nshape des données:\")\n",
    "print(f\"x (features): {x.shape}\")\n",
    "print(f\"y (targets): {y.shape}\")\n",
    "print(f\"sample_weights: {sample_weights.shape}\")\n",
    "\n",
    "# distribution des poids\n",
    "print(f\"\\ndistribution des poids d'échantillons:\")\n",
    "print(pd.Series(sample_weights).value_counts().sort_index())\n",
    "\n",
    "# analyse des phases représentées\n",
    "metadata_df = pd.DataFrame(all_metadata)\n",
    "print(f\"\\nrépartition par phase épidémique dynamique:\")\n",
    "phase_dist = metadata_df['epidemic_phase'].value_counts().sort_index()\n",
    "for phase, count in phase_dist.items():\n",
    "    if phase < len(phase_names):\n",
    "        print(f\"phase {phase} ({phase_names[phase]}): {count} ({count/len(metadata_df)*100:.1f}%)\")\n",
    "\n",
    "# sauvegarde des données préparées\n",
    "np.savez('lstm_sequences.npz', \n",
    "         x=x, y=y, sample_weights=sample_weights,\n",
    "         feature_names=feature_cols,\n",
    "         target_names=target_cols)\n",
    "\n",
    "# sauvegarde des scalers\n",
    "import joblib\n",
    "joblib.dump(scaler_features, 'scaler_features.pkl')\n",
    "joblib.dump(scaler_targets, 'scaler_targets.pkl')\n",
    "\n",
    "# sauvegarde metadata\n",
    "metadata_df.to_pickle('sequences_metadata.pkl')\n",
    "\n",
    "# sauvegarde du dataframe mis à jour avec les phases\n",
    "df.to_pickle('covid_data_with_phases.pkl')\n",
    "\n",
    "print(f\"\\nfichiers sauvegardés:\")\n",
    "print(\"- lstm_sequences.npz\")\n",
    "print(\"- scaler_features.pkl\")\n",
    "print(\"- scaler_targets.pkl\")\n",
    "print(\"- sequences_metadata.pkl\")\n",
    "print(\"- covid_data_with_phases.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330e183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 6 - Split des données\n",
    "\n",
    "\n",
    "# chargement des données\n",
    "data = np.load('lstm_sequences.npz')\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "sample_weights = data['sample_weights']\n",
    "feature_names = data['feature_names']\n",
    "target_names = data['target_names']\n",
    "\n",
    "# correction des poids négatifs\n",
    "print(\"correction des poids d'échantillons...\")\n",
    "sample_weights = np.abs(sample_weights)  # valeur absolue\n",
    "sample_weights[sample_weights == 0] = 0.1  # poids minimum\n",
    "\n",
    "# chargement des métadonnées\n",
    "metadata_df = pd.read_pickle('sequences_metadata.pkl')\n",
    "\n",
    "# Utilisation du split temporel simple pour la rapidité\n",
    "print(\"\\nCréation du split temporel simple...\")\n",
    "n_samples = len(x)\n",
    "train_end = int(0.7 * n_samples)\n",
    "val_end = int(0.85 * n_samples)\n",
    "\n",
    "train_idx = np.arange(0, train_end)\n",
    "val_idx = np.arange(train_end, val_end)\n",
    "test_idx = np.arange(val_end, n_samples)\n",
    "\n",
    "print(f\"\\nSplit temporel:\")\n",
    "print(f\"  train: {len(train_idx)} échantillons\")\n",
    "print(f\"  val: {len(val_idx)} échantillons\") \n",
    "print(f\"  test: {len(test_idx)} échantillons\")\n",
    "\n",
    "x_train, y_train, w_train = x[train_idx], y[train_idx], sample_weights[train_idx]\n",
    "x_val, y_val, w_val = x[val_idx], y[val_idx], sample_weights[val_idx]\n",
    "x_test, y_test, w_test = x[test_idx], y[test_idx], sample_weights[test_idx]\n",
    "\n",
    "# sauvegarde\n",
    "np.savez('train_val_test_split.npz',\n",
    "         x_train=x_train, y_train=y_train, w_train=w_train,\n",
    "         x_val=x_val, y_val=y_val, w_val=w_val,\n",
    "         x_test=x_test, y_test=y_test, w_test=w_test,\n",
    "         train_idx=train_idx, val_idx=val_idx, test_idx=test_idx)\n",
    "\n",
    "print(f\"\\nfichier sauvegardé: train_val_test_split.npz\")\n",
    "print(f\"shapes finales:\")\n",
    "print(f\"  train: {x_train.shape}\")\n",
    "print(f\"  val: {x_val.shape}\")\n",
    "print(f\"  test: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ecd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 7 - Architecture du modèle (VERSION SIMPLIFIÉE)\n",
    "\n",
    "\n",
    "# chargement des données\n",
    "data = np.load('train_val_test_split.npz')\n",
    "x_train, y_train, w_train = data['x_train'], data['y_train'], data['w_train']\n",
    "x_val, y_val, w_val = data['x_val'], data['y_val'], data['w_val']\n",
    "x_test, y_test, w_test = data['x_test'], data['y_test'], data['w_test']\n",
    "\n",
    "print(f\"dimensions des données:\")\n",
    "print(f\"train: {x_train.shape}, poids moyen: {w_train.mean():.3f}\")\n",
    "print(f\"val: {x_val.shape}, poids moyen: {w_val.mean():.3f}\")\n",
    "print(f\"test: {x_test.shape}, poids moyen: {w_test.mean():.3f}\")\n",
    "\n",
    "print(\"\\nVérification de la variance des targets:\")\n",
    "for i, name in enumerate(['mortality_rate', 'transmission_rate', 'spatial_spread']):\n",
    "    print(f\"{name}: mean={y_train[:,i].mean():.4f}, std={y_train[:,i].std():.4f}\")\n",
    "\n",
    "# ARCHITECTURE SIMPLE ET ROBUSTE\n",
    "model = tf.keras.Sequential([\n",
    "    # LSTM simple\n",
    "    tf.keras.layers.LSTM(64, input_shape=(x_train.shape[1], x_train.shape[2]), \n",
    "                        dropout=0.2, recurrent_dropout=0.1),\n",
    "    \n",
    "    # Dense layers\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    \n",
    "    # Sortie linéaire\n",
    "    tf.keras.layers.Dense(3, activation='linear')\n",
    "])\n",
    "\n",
    "# Compilation\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae'],\n",
    "    weighted_metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\nArchitecture du modèle:\")\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "callbacks_list = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.keras',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Monitor simple\n",
    "class SimpleMonitor(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:\n",
    "            val_pred = self.model.predict(x_val[:100], verbose=0)\n",
    "            print(f\"\\nEpoch {epoch} - Std des prédictions: {val_pred.std(axis=0)}\")\n",
    "\n",
    "custom_callback = SimpleMonitor()\n",
    "callbacks_list.append(custom_callback)\n",
    "\n",
    "training_callbacks = callbacks_list\n",
    "\n",
    "print(\"\\nPrêt pour l'entraînement!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELLULE 8 - Entraînement\n",
    "\n",
    "\n",
    "# configuration de l'entraînement\n",
    "batch_size = 128\n",
    "epochs = 30\n",
    "\n",
    "print(f\"configuration de l'entraînement:\")\n",
    "print(f\"- batch size: {batch_size}\")\n",
    "print(f\"- epochs max: {epochs}\")\n",
    "print(f\"- échantillons par epoch: {len(x_train)}\")\n",
    "print(f\"- steps par epoch: {len(x_train) // batch_size}\")\n",
    "\n",
    "# entraînement avec poids d'échantillons\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=(x_val, y_val, w_val),\n",
    "    sample_weight=w_train,\n",
    "    callbacks=training_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# sauvegarde de l'historique\n",
    "history_dict = history.history\n",
    "import pickle\n",
    "with open('training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history_dict, f)\n",
    "\n",
    "print(\"\\n\\nentraînement terminé.\")\n",
    "print(f\"meilleur val_loss: {min(history_dict['val_loss']):.4f}\")\n",
    "print(f\"epoch du meilleur modèle: {np.argmin(history_dict['val_loss']) + 1}\")\n",
    "\n",
    "# visualisation rapide\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# loss\n",
    "axes[0].plot(history_dict['loss'], label='train')\n",
    "axes[0].plot(history_dict['val_loss'], label='validation')\n",
    "axes[0].set_title('Loss (MSE)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# mae\n",
    "axes[1].plot(history_dict['mae'], label='train')\n",
    "axes[1].plot(history_dict['val_mae'], label='validation')\n",
    "axes[1].set_title('MAE')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('MAE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fb5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CELLULE 9 - Évaluation finale\n",
    "\n",
    "\n",
    "# chargement du meilleur modèle\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('best_model.keras')\n",
    "print(\"modèle chargé depuis best_model.keras\")\n",
    "\n",
    "# chargement des données de test\n",
    "data = np.load('train_val_test_split.npz')\n",
    "x_test, y_test, w_test = data['x_test'], data['y_test'], data['w_test']\n",
    "\n",
    "# chargement des scalers pour dénormaliser\n",
    "scaler_targets = joblib.load('scaler_targets.pkl')\n",
    "\n",
    "# prédictions sur les 3 ensembles\n",
    "print(\"\\névaluation sur tous les ensembles...\")\n",
    "x_train, y_train, w_train = data['x_train'], data['y_train'], data['w_train']\n",
    "x_val, y_val, w_val = data['x_val'], data['y_val'], data['w_val']\n",
    "\n",
    "# prédictions (normalisées)\n",
    "y_pred_train = model.predict(x_train, verbose=0)\n",
    "y_pred_val = model.predict(x_val, verbose=0)\n",
    "y_pred_test = model.predict(x_test, verbose=0)\n",
    "\n",
    "# DÉNORMALISATION\n",
    "print(\"\\ndénormalisation des prédictions et targets...\")\n",
    "y_train_denorm = scaler_targets.inverse_transform(y_train)\n",
    "y_val_denorm = scaler_targets.inverse_transform(y_val)\n",
    "y_test_denorm = scaler_targets.inverse_transform(y_test)\n",
    "\n",
    "y_pred_train_denorm = scaler_targets.inverse_transform(y_pred_train)\n",
    "y_pred_val_denorm = scaler_targets.inverse_transform(y_pred_val)\n",
    "y_pred_test_denorm = scaler_targets.inverse_transform(y_pred_test)\n",
    "\n",
    "# calcul des métriques sur valeurs réelles\n",
    "def calculate_metrics(y_true, y_pred, weights, set_name):\n",
    "    \"\"\"calcule les métriques pondérées pour un ensemble\"\"\"\n",
    "    metrics = {'set': set_name}\n",
    "    \n",
    "    # métriques globales\n",
    "    mse = np.average((y_true - y_pred)**2, weights=weights, axis=0).mean()\n",
    "    mae = np.average(np.abs(y_true - y_pred), weights=weights, axis=0).mean()\n",
    "    metrics['mse'] = mse\n",
    "    metrics['mae'] = mae\n",
    "    metrics['rmse'] = np.sqrt(mse)\n",
    "    \n",
    "    # métriques par target\n",
    "    target_names = ['mortality_rate', 'transmission_rate', 'spatial_spread']\n",
    "    for i, target in enumerate(target_names):\n",
    "        target_mae = np.average(np.abs(y_true[:, i] - y_pred[:, i]), weights=weights)\n",
    "        target_rmse = np.sqrt(np.average((y_true[:, i] - y_pred[:, i])**2, weights=weights))\n",
    "        metrics[f'{target}_mae'] = target_mae\n",
    "        metrics[f'{target}_rmse'] = target_rmse\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# calcul pour chaque ensemble\n",
    "train_metrics = calculate_metrics(y_train_denorm, y_pred_train_denorm, w_train, 'train')\n",
    "val_metrics = calculate_metrics(y_val_denorm, y_pred_val_denorm, w_val, 'validation')\n",
    "test_metrics = calculate_metrics(y_test_denorm, y_pred_test_denorm, w_test, 'test')\n",
    "\n",
    "# affichage des résultats\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RÉSULTATS FINAUX - Valeurs dénormalisées\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "metrics_df = pd.DataFrame([train_metrics, val_metrics, test_metrics])\n",
    "print(\"\\nMétriques globales:\")\n",
    "print(metrics_df[['set', 'mae', 'rmse']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\\nMétriques par target (MAE):\")\n",
    "for target in ['mortality_rate', 'transmission_rate', 'spatial_spread']:\n",
    "    print(f\"\\n{target}:\")\n",
    "    for metrics in [train_metrics, val_metrics, test_metrics]:\n",
    "        print(f\"  {metrics['set']:12s}: {metrics[f'{target}_mae']:.4f}\")\n",
    "\n",
    "# test de généralisation\n",
    "val_mae = val_metrics['mae']\n",
    "test_mae = test_metrics['mae']\n",
    "degradation = (test_mae - val_mae) / val_mae * 100\n",
    "\n",
    "print(f\"\\n\\nTest de généralisation:\")\n",
    "print(f\"MAE validation: {val_mae:.4f}\")\n",
    "print(f\"MAE test: {test_mae:.4f}\")\n",
    "print(f\"Dégradation: {degradation:+.1f}%\")\n",
    "\n",
    "if degradation < 20:\n",
    "    print(\" Généralisation acceptable\")\n",
    "else:\n",
    "    print(\" Dégradation importante\")\n",
    "\n",
    "# visualisation simple\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "target_names = ['mortality_rate', 'transmission_rate', 'spatial_spread']\n",
    "\n",
    "for i, target in enumerate(target_names):\n",
    "    ax = axes[i]\n",
    "    ax.scatter(y_test_denorm[:, i], y_pred_test_denorm[:, i], \n",
    "              c=w_test, cmap='viridis', alpha=0.5, s=10)\n",
    "    \n",
    "    # ligne diagonale\n",
    "    min_val = min(y_test_denorm[:, i].min(), y_pred_test_denorm[:, i].min())\n",
    "    max_val = max(y_test_denorm[:, i].max(), y_pred_test_denorm[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel(f'Vrai {target}')\n",
    "    ax.set_ylabel(f'Prédit {target}')\n",
    "    ax.set_title(f'{target} (test set)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('test_evaluation_final.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# rapport final\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAPPORT FINAL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Performance globale MAE test: {test_metrics['mae']:.4f}\")\n",
    "print(f\"Meilleure prédiction: {min(test_metrics['mortality_rate_mae'], test_metrics['transmission_rate_mae'], test_metrics['spatial_spread_mae']):.4f}\")\n",
    "print(f\"Pire prédiction: {max(test_metrics['mortality_rate_mae'], test_metrics['transmission_rate_mae'], test_metrics['spatial_spread_mae']):.4f}\")\n",
    "\n",
    "# sauvegarde des résultats\n",
    "with open('final_results.txt', 'w') as f:\n",
    "    f.write(f\"MAE Test: {test_metrics['mae']:.4f}\\n\")\n",
    "    f.write(f\"Mortality Rate MAE: {test_metrics['mortality_rate_mae']:.4f}\\n\")\n",
    "    f.write(f\"Transmission Rate MAE: {test_metrics['transmission_rate_mae']:.4f}\\n\")\n",
    "    f.write(f\"Spatial Spread MAE: {test_metrics['spatial_spread_mae']:.4f}\\n\")\n",
    "\n",
    "print(\"\\nFichiers sauvegardés: test_evaluation_final.png, final_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
